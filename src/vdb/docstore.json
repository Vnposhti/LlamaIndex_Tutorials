{"docstore/metadata": {"7392c46d-5b4d-436e-a7c1-b3283f21c096": {"doc_hash": "13b7f0476d64b28417fe1ec52beaf49877c74006812720d614befb0accec9c07"}, "f648df64-0a96-4e99-a75f-bc250606c95f": {"doc_hash": "b9704ce549fe45be57979841c06e68f3923c5122328fc6effdee7b825ad3fe74"}, "55b33995-c5a4-4fd0-b273-d624f524fe2b": {"doc_hash": "32927bcc3c97ccf9d13f64d0b958e0b76560c2bc8581813b976e92f3f3a2c12f"}, "bad7ee9f-8397-49c7-aca1-22da2107d17c": {"doc_hash": "dae5c79764e60a22624d727cefca31d0f64c0ca63fe19ef106fbbbb1975b2ca1"}, "69a74af0-200c-4f3d-9995-8d9461dbe8b1": {"doc_hash": "f65710c7fb1a968babc05b4a515980af551655ac07c6974090bfbaf4fabcaedd"}, "47440797-be04-4c3e-92bf-e6a0f1d16040": {"doc_hash": "e4d3240ef7ddcb1f95b671b1935d1ab0c189c2e54dfcad13e65610493cbbcb84"}, "fa508c27-f9ab-4b63-8508-5a1baeb708c4": {"doc_hash": "dc6d63c3d9957b47c74be5059e15c22db3ddbef5844acb9efa307969e2fcef43"}, "4f968278-465f-4073-9468-bc9beb3839bb": {"doc_hash": "bd3f74c106837d1eaa58ca96adc19b31f7a7f41dc2130de44414128004b68b47"}, "8288ff90-f499-40b7-bfd9-55647dabd13e": {"doc_hash": "eccd7fd8e4fa8306db5393a98ddc81952f7405f8ef8930f5f82c9110200a4393"}, "6ca99dcc-f0c1-49d3-8aeb-180b877f2a00": {"doc_hash": "a5f7008900f75e913b2416e77ece6b1481a3cb29e70b056248e5c1e8511d9dd0"}, "772b9db4-473c-490d-b7bd-8dd33cf61f11": {"doc_hash": "4c59b1bf54bc564ead8ee14a7394982957d60638144c129167e9d9009472463f"}, "4a2cda1d-1a27-48a1-ba21-2be43b90da91": {"doc_hash": "7578a650cda500fc9655a78a3682e9c96a44e826297b431753be4d20c2209092"}, "96b53d13-6ac1-40da-aa24-1f8afe0ecfcb": {"doc_hash": "97fade57a7fb1b55716855128b72026cf17a03324450f0da830bd8c7dfc258d0"}, "b78ea85e-05c7-4b25-86fb-89793ca403c8": {"doc_hash": "33304076220aa66c56aa44ab83219e1dad2977e438620385c4b1356e5c68d7f4"}, "28e732bb-d22b-47a5-b332-a8b6634faf14": {"doc_hash": "0a599e91565be94d04b5199388e6ad47f712b267d2b0430f3f47b40538dede0c"}, "e7020f88-8394-467b-9b6a-5c611ef616e2": {"doc_hash": "dd3a7f9f5e95021b36d17f2706ad4a0ed36b38e0bd01ca0e6f17293ef2493eb1"}, "adbfeec2-5a5f-4ff4-9aef-1474496dc35e": {"doc_hash": "c8019f5d2129d06ef3654dc2659f8449d1e6285fb5338689a141046ba0ed9bef"}, "10a790e5-5165-4b89-bd06-8e554e694b9d": {"doc_hash": "77771865424f264dd6d23a2a50232562d02be2bcac6127856bb2b3dcb8730287"}, "4611a9e2-15bf-402e-9373-af165e09db4c": {"doc_hash": "d488e62ef2636cb52e545a650b2c51900841ea7d7c503e381bcc23da6e895a80"}, "23cc4249-e09f-49b7-9dc6-02d5669dfc39": {"doc_hash": "844f0c8da16514577f3bc43ce0fdbd2ed8ece12d6a1c7e73bd52a4af431a7ca8"}, "1c34b811-bb5c-4b74-a7c8-5132e5f0fbd8": {"doc_hash": "cba71ad46e755d65a5e2df3f62f3c0f14adea0ea7356b72ef29aa1c959624745"}, "523d422c-6f89-49f0-8548-2ceb6209177a": {"doc_hash": "03cdf40c318fb8b12b4098108a43fbe2303669d90d58259859f48c284e480a40"}, "f22d37ed-ada0-4778-9e4d-4588c832f236": {"doc_hash": "caf8f3d3af934ff565e9131c1068becfbc8327bd75f3bcea0362c51bed4fb900"}, "808a7ae9-6b1a-41ee-b194-63087b7fdcb7": {"doc_hash": "c3da97857af5d02f06bc19a9a7663734f855f530cf009f0e0c7f8d912db0b7fd"}, "8805232e-3503-418b-b19e-e81313da17ec": {"doc_hash": "54839b74c85c7c7f91830df0984cfac47959640b7f5a9171793513473494bb77"}, "35d354fc-bc8b-46b6-8c25-6bd0de7c4bbe": {"doc_hash": "b69b2ced6ecf7404917e9bde9deb9c9d5065e5d6496b90905bc6fb6028bf6a65"}, "cbeecace-e833-4199-8735-5a0bb1513094": {"doc_hash": "ca47561df5d2c4b765ec3415270006e1df06c0fe5dd0784ceb7f6a8aea264c47"}, "6f3147e3-bb89-45f3-96da-190140a2d457": {"doc_hash": "48228f484c60a039c22267535e14e915863729d456d87ce8117eeab2064bfdf3"}, "729179ed-47b7-4fae-a665-f616a811afd9": {"doc_hash": "316c7054f77c3a8e2d9471204c414fdddb1556bd3427158dcb05016f6648109f"}, "22ad2972-52c0-45df-9571-738f1682ff77": {"doc_hash": "b70d2e7ab012f113de8631998fc0a003b38877a3eee6b98927ebc09e5ca05f8c"}, "7640c971-d974-4a3c-a75b-072b5008ec05": {"doc_hash": "01a737d284beb7f99b36bba78607e19f8126dab04ab5f2a016e678d4db008617", "ref_doc_id": "7392c46d-5b4d-436e-a7c1-b3283f21c096"}, "04ccfe3c-9f63-4c80-afbd-de2a307aa69c": {"doc_hash": "b1cc7bd52ba45505fb0a72cd14279896fe626fb00b3a46e8d502e4d44610e518", "ref_doc_id": "f648df64-0a96-4e99-a75f-bc250606c95f"}, "093521af-dd8f-4619-9022-e5eada65779a": {"doc_hash": "acddb9e671b013b2687a4967bdb6cb8c00ccc7a49d375e45cf602e9c1ca158b3", "ref_doc_id": "55b33995-c5a4-4fd0-b273-d624f524fe2b"}, "e42b80bb-1d8a-4683-b0f2-cb990c8103d6": {"doc_hash": "8c958eb257750d1d8fa01f37a9886926e2452da48c6fbf6f6cbd7a1b418aaf32", "ref_doc_id": "bad7ee9f-8397-49c7-aca1-22da2107d17c"}, "cf0809ed-8cec-4993-b359-5f6dc8e9c4c1": {"doc_hash": "ca1990d874281ba7d0d6c6b302de1d0402040e3750bdbbf7626523029410fa07", "ref_doc_id": "69a74af0-200c-4f3d-9995-8d9461dbe8b1"}, "a210186a-4ca8-49b8-87e2-273595024777": {"doc_hash": "285ee3071f447317bebd1fba95f2e0adb3d3ed15c6006cf83f87037c0c4eeb0f", "ref_doc_id": "47440797-be04-4c3e-92bf-e6a0f1d16040"}, "3d199ae9-9787-41d4-bde5-efd220aee357": {"doc_hash": "957f39f474519f2efcd0ac8dc811e8054b1ac71204cbed81793a4881d88c46d4", "ref_doc_id": "fa508c27-f9ab-4b63-8508-5a1baeb708c4"}, "3048529c-f4aa-4266-bd70-ef21f4f9bc47": {"doc_hash": "124753e90435443f98ec4c2cfd706d1c5debf806ffef08310e675c4638d75444", "ref_doc_id": "4f968278-465f-4073-9468-bc9beb3839bb"}, "92c049e4-0b98-4564-baf9-c63d186125b3": {"doc_hash": "ef8208b961812aef25b56ffb767c548b6f4d36060be3bf6df5d6053f4ae0d6e8", "ref_doc_id": "8288ff90-f499-40b7-bfd9-55647dabd13e"}, "8476e1b3-b9cc-427f-bd26-dae9013263d4": {"doc_hash": "65fd594bb95243e79847e1434ec471c2197c1c9298dcd1ebf55cf6b49321fc1b", "ref_doc_id": "6ca99dcc-f0c1-49d3-8aeb-180b877f2a00"}, "d844248f-f515-4da4-94e1-26e939335820": {"doc_hash": "11898db7279d343be03b8ff0982921c99eef0cde00165e53878009708cd0d547", "ref_doc_id": "772b9db4-473c-490d-b7bd-8dd33cf61f11"}, "1558bd5a-19c5-4ce9-8a51-a391588ddfcd": {"doc_hash": "0115487a562241b3780bf18af0d8db158ff8de6a2206f230bf411ab1fd150b79", "ref_doc_id": "4a2cda1d-1a27-48a1-ba21-2be43b90da91"}, "fc731157-a527-4712-be13-e352c3567a31": {"doc_hash": "1b62db8648aa13f4b1d40dfd1813d7c749c6cdea8577e439f5a71752bf80ff21", "ref_doc_id": "96b53d13-6ac1-40da-aa24-1f8afe0ecfcb"}, "aed5196b-e68a-47f4-b9a8-f59785ccce55": {"doc_hash": "bf29ad2e820be3fed003b67dff63cca54dd4b4738dec30e91b3b9b0ed0e3cd0c", "ref_doc_id": "b78ea85e-05c7-4b25-86fb-89793ca403c8"}, "e6887d47-75c4-4050-8d98-53fe3e232c13": {"doc_hash": "0b3012c62981fa6ccd14418c892435f7b595241491e03583cdcfb498a6112164", "ref_doc_id": "28e732bb-d22b-47a5-b332-a8b6634faf14"}, "7a58867c-9dfd-4b7b-ac50-14bc9b1fdabc": {"doc_hash": "5a1e31c9feacab3f4f2b1c74609e7d89e2e965900dc931a561f286a79d8c2aef", "ref_doc_id": "e7020f88-8394-467b-9b6a-5c611ef616e2"}, "25439d0e-a06a-4b07-83e2-944bdca8690e": {"doc_hash": "75afb73ca1b73ee09a24ba0ccb664dc33c779a9adcbdcbbb35cbe3f78ed1f8e6", "ref_doc_id": "adbfeec2-5a5f-4ff4-9aef-1474496dc35e"}, "13e9062c-84eb-4aba-bde6-87032e8411c3": {"doc_hash": "0185c616693215477d2c4d3366f947f12556efbbef79220261af7b871b78a664", "ref_doc_id": "10a790e5-5165-4b89-bd06-8e554e694b9d"}, "c43d09d4-188a-436f-ac94-8d6ddef65372": {"doc_hash": "49af9914b4c410af993577840c153e116e8062906eb46273064544319b41908f", "ref_doc_id": "4611a9e2-15bf-402e-9373-af165e09db4c"}, "cc8f10f8-93c8-45b1-8e96-909ea9271f51": {"doc_hash": "8b60825d26111487ea8f05a0828ba0ae5c68a1db42c63deb88beee3da2aa0282", "ref_doc_id": "23cc4249-e09f-49b7-9dc6-02d5669dfc39"}, "8972e66c-a028-4e86-829c-ec2c5bf17daf": {"doc_hash": "793c5333f5ebf45955fb5844f3ff7272bb063cd11645f2763afd1d953b1aae47", "ref_doc_id": "1c34b811-bb5c-4b74-a7c8-5132e5f0fbd8"}, "542e88c8-14d9-4d10-962b-9b56f7afec3c": {"doc_hash": "638d9a376c9a5b1863d1b604b6e744a1e89b20be290b1fc7d95300a935ecbd2c", "ref_doc_id": "523d422c-6f89-49f0-8548-2ceb6209177a"}, "bc71aea8-4100-4845-bb25-59571125e140": {"doc_hash": "3423b239c049abd0e61dec5cbd137c82341e0764fbb7426c8a7eb136b4e1aa6e", "ref_doc_id": "f22d37ed-ada0-4778-9e4d-4588c832f236"}, "0b825b6d-a0c3-4b11-9f93-c4a863d4a2ea": {"doc_hash": "9b020e3e2cd202ea7460bd639c5a71ec49afe3e9f56839a967057ffb89f8f3cb", "ref_doc_id": "808a7ae9-6b1a-41ee-b194-63087b7fdcb7"}, "8081dad5-b35a-4ea0-83d0-9c803016a5de": {"doc_hash": "d1885459a412e0f87e224896091064965ebd126e31c7805afe553eae90ac233b", "ref_doc_id": "8805232e-3503-418b-b19e-e81313da17ec"}, "426f08c2-0273-418a-a0f0-3a6ee9a61fee": {"doc_hash": "897c9fb2b0428c44f56b501a461aa6d7ee8d6820601b7750427fbd3575435ab3", "ref_doc_id": "35d354fc-bc8b-46b6-8c25-6bd0de7c4bbe"}, "d3415939-c466-4dfe-9018-3d21344c007e": {"doc_hash": "e0d3e7ca05498a20a8f8880d9ee48c9883626f55e93e7e30385c808e1ded142b", "ref_doc_id": "cbeecace-e833-4199-8735-5a0bb1513094"}, "9a70d22e-e788-4806-a617-3a8de5a3deb7": {"doc_hash": "4435e8b3e7f54792658f2b7dc66aa69a885a1dba1b63d73db95b487a0d01d7dd", "ref_doc_id": "6f3147e3-bb89-45f3-96da-190140a2d457"}, "636d12f6-e916-45b9-aa9f-bd3e3503851f": {"doc_hash": "8182b6494e666ffaaa0c1dc2bd2c66df12b04a2f3a41025748aa7e2893fea847", "ref_doc_id": "729179ed-47b7-4fae-a665-f616a811afd9"}, "871193e4-41b7-40a4-81bb-f8551cafee12": {"doc_hash": "9b5db4363fbe54df61fae09bd4400b82c733629ab0b47de3686a1baa017cc317", "ref_doc_id": "22ad2972-52c0-45df-9571-738f1682ff77"}}, "docstore/data": {"7640c971-d974-4a3c-a75b-072b5008ec05": {"__data__": {"id_": "7640c971-d974-4a3c-a75b-072b5008ec05", "embedding": null, "metadata": {"page_label": "1", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7392c46d-5b4d-436e-a7c1-b3283f21c096", "node_type": "4", "metadata": {"page_label": "1", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "13b7f0476d64b28417fe1ec52beaf49877c74006812720d614befb0accec9c07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Breakdown of the Imports:\n1. import torch \u2013 This imports PyTorch, which we\u2019ll use for tensor computations, autograd, and \nmodel building.\n2. import torch.nn as nn \u2013 This brings in PyTorch\u2019s neural network module (nn), which contains \nlayers like nn.Linear, nn.Transformer, etc.\n3. import torch.optim as optim \u2013 This imports PyTorch\u2019s optimization library, which helps in training \nthe model (like Adam, SGD).\n4. import gzip \u2013 This is a built-in Python library to handle .gz compressed files. (Maybe we are \ndealing with a compressed dataset?)\n5. import time \u2013 Used for measuring execution time, probably for tracking training speed.\n6. import math \u2013 Might be used for mathematical operations like scaling, exponentials, etc. (often \nused in the Transformer\u2019s positional encoding).\n7. import spacy \u2013 A popular NLP library, often used for tokenization. It helps in processing text (like \nsplitting sentences into words).\n8. from torch.utils.data import Dataset, DataLoader \u2013\n\u25cb Dataset helps define a custom dataset.\n\u25cb DataLoader is used to load data in batches efficiently.\n9. from torch.nn.utils.rnn import pad_sequence \u2013 Helps pad sequences to the same length \n(important for batch processing in NLP models).\n10. from tqdm import tqdm \u2013 A progress bar library, used for displaying progress during training or \ndata processing.\n What This Tells Us About the Code?\n\u2022 This is an NLP-based Transformer model (because of spacy and pad_sequence).\n\u2022 It will involve batch processing (because of Dataset and DataLoader).\n\u2022 It might be trained from scratch (since we are not importing Hugging Face's Transformer library).\n\u2022 Positional encoding or some math-based scaling might be involved (math library).\n\u2022 The dataset might be compressed (gzip), so we\u2019ll likely load .gz files.\n Why Use a Compressed .gz File?\n1.               \u2013  arge te t datasets take up a lot of disk space.  ompressing them reduces \nstorage re uirements.\n2. Faster Loading  \u2013 Instead of storing raw text, compressed files can be read directly while \ntraining, reducing I/O overhead.\n3. Common in NLP Datasets  \u2013 Many public datasets (like WikiText, OpenWebText, or large \ncorpora) come in .gz format to keep downloads manageable.\n4. Efficient for Streaming \u23f3\u2013 We can read data in chunks rather than loading everything into \nmemory at once.\n Ex m l  of Why It\u2019  U  d:\n et\u2019s say we have a dataset of 500MB in raw text. If we compress it using .gz, it might become 50MB, \nmaking it easier to download and load into memory while training.\n Next: If you share the code where the .gz file is actually being read, I can explain exactly how it's \nbeing handled! \n What These Commands Do?\nSunday, February 2, 2025 6:18 PM\n   transformer Page 1", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 2698, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "04ccfe3c-9f63-4c80-afbd-de2a307aa69c": {"__data__": {"id_": "04ccfe3c-9f63-4c80-afbd-de2a307aa69c", "embedding": null, "metadata": {"page_label": "2", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f648df64-0a96-4e99-a75f-bc250606c95f", "node_type": "4", "metadata": {"page_label": "2", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "b9704ce549fe45be57979841c06e68f3923c5122328fc6effdee7b825ad3fe74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What These Commands Do?\n1. !python -m spacy download de_core_news_sm\n\u25cb Downloads the German (de) small language model for spaCy.\n\u25cb \"core_news\" means it\u2019s designed for news-style text.\n\u25cb \"sm\" means small model (faster, but less accurate).\n2. !python -m spacy download en_core_web_sm\n\u25cb Downloads the English (en) small language model.\n\u25cb \"core_web\" means it\u2019s trained on web text (more general-purpose).\n\u25cb \"sm\" means small model, suitable for lightweight processing.\n Why Are We Downloading These?\n\u2022 These models help with tokenization, lemmatization, and POS tagging.\n\u2022 In NLP pipelines, we often need to split text into words, remove unnecessary elements, and \nextract useful features.\n\u2022 Since we are working on a Transformer model, we might need to preprocess text before passing \nit into the model.\n Wh t\u2019  N xt?If your code is using spaCy for tokenization, share that part, and I'll explain exactly \nhow it fits into your Transformer pipeline!  \n What Does This Code Do?\nspacy_de = spacy.load('de_core_news_sm')\nspacy_en = spacy.load('en_core_web_sm')\n\u2022 This loads the pre-trained spaCy language models for German (de) and English (en).\n\u2022 These models contain pre-trained tokenizers, part-of-speech (POS) taggers, lemmatizers, and \nnamed entity recognition (NER) systems.\n\u2022 The variable names (spacy_de and spacy_en) indicate that these will be used for processing \nGerman and English text, respectively.\n Why Are We Using spaCy?\nSince we are building a Transformer model, we need to process text before feeding it into the model. \nspaCy helps us:\n1. Tokenize Sentences \u2013 Converts text into a list of words (tokens).\n2. Normalize Text \u2013 Handles punctuation, special characters, etc.\n3. Lemmatization (if needed) \u2013  onverts words to their base form (e.g., \"running\" \u2192 \"run\").\n4. Language-Specific Rules \u2013 Helps deal with differences in German and English grammar.\n Example of How spaCy Works\ntext = \"Hallo, wie geht es dir?\"\ntokens = [tok.text for tok in spacy_de(text)]\nprint(tokens)\nOutput:\n['Hallo', ',', 'wie', 'geht', 'es', 'dir', '?']\nAs you can see, it splits the sentence into individual words and punctuation, which is important for NLP \ntasks.\n Next Step: Your Transformer code will likely use spaCy for tokenization before converting text into \nnumerical data (embeddings). If that part is next, send it over, and I'll break it down!   \n What Does This Code Do?\n   transformer Page 2", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 2396, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "093521af-dd8f-4619-9022-e5eada65779a": {"__data__": {"id_": "093521af-dd8f-4619-9022-e5eada65779a", "embedding": null, "metadata": {"page_label": "3", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55b33995-c5a4-4fd0-b273-d624f524fe2b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "32927bcc3c97ccf9d13f64d0b958e0b76560c2bc8581813b976e92f3f3a2c12f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What Does This Code Do?\nSEED = 123\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n1. SEED = 123 \n\u25cb This sets a random seed. In machine learning, using a fixed random seed ensures \nreproducibility\u2014so every time you run your code, you'll get the same results. It's essential \nfor debugging and comparing experiments.\n2. torch.manual_seed(SEED) \n\u25cb This sets the seed for PyTorch's random number generator. This ensures that any random \noperations (like weight initialization, shuffling data, etc.) give the same results across runs.\n\u25cb This is especially useful if you're doing experiments or training models, and you want to \nmake sure that your results are consistent.\n3. torch.backends.cudnn.deterministic = True \n\u25cb This is related to CUDA (GPU). When training on a GPU, some operations can be non-\ndeterministic (i.e., produce slightly different results each time due to optimization for \nspeed).\n\u25cb By setting this flag to True, we make the operations deterministic on NVIDIA GPUs using \ncuDNN, ensuring the results are exactly the same every time (given the same input, same \nseed).\n Why Is This Important?\n\u2022 Reproducibility is a key aspect of machine learning. Setting a fixed seed ensures that: \n1. If you or anyone else runs the code multiple times, you'll get consistent results (same \noutput, same metrics).\n2. Helps debug effectively, because if there\u2019s a bug or performance drop, you can isolate it and \nknow it's not due to randomness.\n\u2022 CUDA Determinism: When working on GPUs, it's easy to get tiny differences in results because of \nparallel operations. This setting ensures exact replication of results, making it easier to compare \nexperiments.\n Next Step: If you have more code that involves model building or \ntraining, feel free to share it! I can help explain how all these pieces \ncome together in the Transformer model.  \n What Does This Code Do?\nThis part of the code defines a custom Dataset class called Multi30kDataset. This class is responsible for \nloading, transforming, and providing access to the data in a convenient way for training the model.\n1. class Multi30kDataset(Dataset):\n\u2022 This is creating a custom dataset class that inherits from torch.utils.data.Dataset, which is the \nstandard way of working with datasets in PyTorch.\n\u2022 The Multi30k dataset is likely related to machine translation, where the source and target \nlanguages are provided in separate files.\n2. def __init__(self, src_file, trg_file, src_transform=None, trg_transform=None):\n\u2022 Constructor to initialize the dataset. \n\u25cb src_file: Path to the source language file (e.g., German).\n\u25cb trg_file: Path to the target language file (e.g., English).\n\u25cb src_transform: Optional transformation for the source sentences (like tokenization or \npadding).\ntrg_transform: Optional transformation for the target sentences.\n   transformer Page 3", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 2849, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e42b80bb-1d8a-4683-b0f2-cb990c8103d6": {"__data__": {"id_": "e42b80bb-1d8a-4683-b0f2-cb990c8103d6", "embedding": null, "metadata": {"page_label": "4", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bad7ee9f-8397-49c7-aca1-22da2107d17c", "node_type": "4", "metadata": {"page_label": "4", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "dae5c79764e60a22624d727cefca31d0f64c0ca63fe19ef106fbbbb1975b2ca1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u25cb trg_transform: Optional transformation for the target sentences.\n3. self.src_data = self.load_data(src_file) and self.trg_data = self.load_data(trg_file)\n\u2022 Loading the Data: \n\u25cb The function load_data is called for both the source (src_file) and target (trg_file) data files.\n\u25cb The gzip.open method reads the compressed files (.gz format), and the data is loaded line-\nby-line as a list of sentences.\n\u25cb The strip() method removes any unnecessary whitespace or newline characters from the \nsentences.\n4. def load_data(self, file_path):\n\u2022 This method opens and reads the contents of the dataset files.\n\u2022 The dataset is expected to be one sentence per line in the .gz compressed files.\n5. def __len__(self):\n\u2022 Returns the length of the dataset, i.e., the number of source sentences (assuming source and \ntarget files are of equal length).\n6. def __getitem__(self, idx):\n\u2022 The getitem method allows us to index into the dataset and get a specific item (sentence pair) by \nits index idx.\n\u2022 It returns a dictionary with: \n\u25cb \"src\": The source sentence.\n\u25cb \"trg\": The target sentence.\n\u2022 Transforms: If any transformations (like tokenization, lowercasing, or padding) were provided, \nthey are applied to both the source and target sentences before returning them.\n Why Is This Important?\n1. Custom Dataset Class: This class makes it easy to work with custom data by using PyTorch\u2019s \nDataLoader. The dataset can be used in training loops, and since it supports transformations, you \ncan preprocess data (like tokenization or padding) before it reaches the model.\n2. Efficient Data Handling:\n\u25cb The data is read line by line from a compressed file, saving both disk space and loading \ntime.\n\u25cb By using gzip and transformations, you ensure that the data is ready for feeding into a \nneural network without extra overhead.\n3. Language Pairs: This is likely part of a machine translation task where the source language (e.g., \nGerman) is mapped to a target language (e.g., English).\n Next Step:\nIf you're using this dataset for a Transformer, you'll likely need to tokenize these sentences and convert \nthem into numerical formats (like word IDs). If there's more code, share it, and I can help explain how \neverything fits together!  \nSteps to Run the Code:\n1. Install the Required Libraries:\n\u25cb You need torch and spaCy for this to work. Install them using pip if you haven\u2019t already: \npip install torch spacy tqdm\n2. Download the Language Models:\n\u25cb Download the language models as per your earlier code: \npython -m spacy download de_core_news_sm\npython -m spacy download en_core_web_sm\n3. Prepare Sample Data:\n\u25cb If you don\u2019t have the actual .gz files (src_file and trg_file), you can create dummy text files\nfor testing: \n   transformer Page 4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cf0809ed-8cec-4993-b359-5f6dc8e9c4c1": {"__data__": {"id_": "cf0809ed-8cec-4993-b359-5f6dc8e9c4c1", "embedding": null, "metadata": {"page_label": "5", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "69a74af0-200c-4f3d-9995-8d9461dbe8b1", "node_type": "4", "metadata": {"page_label": "5", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "f65710c7fb1a968babc05b4a515980af551655ac07c6974090bfbaf4fabcaedd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "for testing: \n\u25aa Create two simple text files src.txt (for German) and trg.txt (for English).\n\u25aa Put a few sentences in each file to simulate the dataset (e.g., German sentence in \nsrc.txt and corresponding English sentence in trg.txt).\n4. Code for Testing the Dataset:\n\u25cb Here\u2019s how you can test it with sample files: \nimport torch\nimport spacy\nfrom torch.utils.data import DataLoader\n# Load the language models\nspacy_de = spacy.load('de_core_news_sm')\nspacy_en = spacy.load('en_core_web_sm')\n# Define a simple transformation (e.g., tokenize)\ndef tokenize_de(sentence):\n    return [token.text for token in spacy_de(sentence)]\ndef tokenize_en(sentence):\n    return [token.text for token in spacy_en(sentence)]\n# Assuming you have `src.txt` and `trg.txt` files\nsrc_file = 'src.txt'  # Replace with actual path\ntrg_file = 'trg.txt'  # Replace with actual path\n# Create the dataset\ndataset = Multi30kDataset(src_file, trg_file, src_transform=tokenize_de, \ntrg_transform=tokenize_en)\n# Create a DataLoader to batch and shuffle the dataset\ndata_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n# Fetch one batch of data\nfor batch in data_loader:\n    print(batch)\n5. Run the Code:\n\u25cb Run the script in your Python environment. It will: \n1. Load the dataset.\n2. Apply tokenization using spaCy.\n3. Print out a batch of tokenized sentences.\nExpected Output:\nAfter running the script, you should see output like this (example with a batch size of 2):\n{'src': [['Hallo', ',', 'wie', 'geht', 'es', 'dir', '?'], ['Guten', 'Morgen', '!']], 'trg': [['Hello', ',', 'how', 'are', 'you', \n'?'], ['Good', 'morning', '!']]}\n Tip:\nIf you encounter issues, feel free to share the error message here, and I'll help you debug it!\n What These Functions Do?\ndef tokenize_de(text):\n    return [token.text.lower() for token in spacy_de.tokenizer(text)]\n   transformer Page 5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a210186a-4ca8-49b8-87e2-273595024777": {"__data__": {"id_": "a210186a-4ca8-49b8-87e2-273595024777", "embedding": null, "metadata": {"page_label": "6", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47440797-be04-4c3e-92bf-e6a0f1d16040", "node_type": "4", "metadata": {"page_label": "6", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "e4d3240ef7ddcb1f95b671b1935d1ab0c189c2e54dfcad13e65610493cbbcb84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "return [token.text.lower() for token in spacy_de.tokenizer(text)]\ndef tokenize_en(text):\n    return [token.text.lower() for token in spacy_en.tokenizer(text)]\n1. tokenize_de(text)\n\u2022 This function takes a German text (text) as input and processes it through the spaCy tokenizer for \nGerman (spacy_de).\n\u2022 .tokenizer(text): Tokenizes the sentence into individual words or subword units based on spaCy\u2019s \nbuilt-in rules.\n\u2022 .lower(): Converts every token to lowercase, ensuring that the model doesn\u2019t treat words like \n\"Hello\" and \"hello\" as separate entities. This can help in reducing vocabulary size and improving \ngeneralization.\n2. tokenize_en(text)\n\u2022 This is the same as tokenize_de, but it works for English text using the English tokenizer \n(spacy_en).\n Why Lowercasing?\n\u2022 Consistency: Lowercasing makes sure that the model doesn't treat words with different cases \n(e.g., \"Apple\" and \"apple\") as separate entities. This is particularly useful for small datasets or \nwhen you don't want to overfit to the case of words.\n\u2022 Simplification: Helps reduce the size of the vocabulary, as the model doesn\u2019t have to learn \nseparate embeddings for upper- and lowercase versions of the same word.\n Example:\nLet's assume we pass a sentence like \"Hallo, Wie geht es dir?\" through tokenize_de and \"Hello, how are \nyou?\" through tokenize_en.\n# Test tokenization\ngerman_text = \"Hallo, Wie geht es dir?\"\nenglish_text = \"Hello, how are you?\"\nprint(tokenize_de(german_text))  # Output: ['hallo', ',', 'wie', 'geht', 'es', 'dir', '?']\nprint(tokenize_en(english_text))  # Output: ['hello', ',', 'how', 'are', 'you', '?']\nNext Step:\nIf you're using these functions in your Multi30kDataset class (as the transform functions), your dataset \nwill now return tokenized and lowercase sentences. If you have more parts of the code (like padding, \nbatch processing, or model definitions), feel free to share them! I'll help explain how they all work \ntogether.  \n What Does This Code Do?\n1. Loading Datasets:\ntrain_data = Multi30kDataset(train_de_path, train_en_path, src_transform=tokenize_de, \ntrg_transform=tokenize_en)\nval_data = Multi30kDataset(val_de_path, val_en_path, src_transform=tokenize_de, \ntrg_transform=tokenize_en)\ntest_data = Multi30kDataset(test_de_path, test_en_path, src_transform=tokenize_de, \ntrg_transform=tokenize_en)\n\u2022 Loading Datasets: \n\u25cb These lines are creating three instances of the Multi30kDataset class for the training, \nvalidation, and test datasets.\n\u25cb train_de_path, train_en_path: These variables store the paths to the German and English\ntraining data files (same for validation and test data).\n\u25cb src_transform=tokenize_de: This applies the tokenize_de function to the German \nsentences. This means every sentence in the source language will be tokenized into \nindividual words and converted to lowercase.\n   transformer Page 6", "mimetype": "text/plain", "start_char_idx": 4, "end_char_idx": 2843, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d199ae9-9787-41d4-bde5-efd220aee357": {"__data__": {"id_": "3d199ae9-9787-41d4-bde5-efd220aee357", "embedding": null, "metadata": {"page_label": "7", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa508c27-f9ab-4b63-8508-5a1baeb708c4", "node_type": "4", "metadata": {"page_label": "7", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "dc6d63c3d9957b47c74be5059e15c22db3ddbef5844acb9efa307969e2fcef43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "individual words and converted to lowercase.\n\u25cb trg_transform=tokenize_en: Similarly, the tokenize_en function is applied to the English \nsentences.\nResult: After loading the dataset, the Multi30kDataset class will return a list of tokenized \nsentences in both German and English when you index into it.\n2. Defining Special Tokens:\nPAD_TOKEN = '<pad>'\nSOS_TOKEN = '<sos>'\nEOS_TOKEN = '<eos>'\nUNK_TOKEN = '<unk>'\nThese lines define special tokens that will be used during the model training and tokenization process. \nLet's break them down:\n1. PAD_TOKEN = '<pad>':\n\u25cb This is the padding token. Padding is used when sentences of varying lengths are batched \ntogether for training.\n\u25cb The model uses the padding token to \"fill in\" shorter sequences, so all sequences in the \nbatch are of equal length (usually the length of the longest sentence in the batch).\n2. SOS_TOKEN = '<sos>':\n\u25cb This stands for Start Of Sequence. It is typically added at the beginning of a sequence in \nsequence-to-sequence models (like machine translation) to signal the start of the target \nsentence.\n\u25cb In machine translation, the model generates tokens one by one, starting with the <sos> \ntoken.\n3. EOS_TOKEN = '<eos>':\n\u25cb This stands for End Of Sequence. It is used to mark the end of a generated sequence.\n\u25cb When the model predicts the <eos> token, it stops generating further tokens.\n4. UNK_TOKEN = '<unk>':\n\u25cb This stands for Unknown. It's used when the model encounters a word that is not in its \nvocabulary.\n\u25cb The model replaces any unknown or rare words with this token, preventing errors during \nprocessing.\n Why Are These Special Tokens Important?\n\u2022 Padding (<pad>): Ensures that all sequences in a batch have the same length, making them easier \nto process in parallel. It's ignored by the model during training.\n\u2022 Start and End Tokens (<sos>, <eos>): These help the model know when to start and stop \ngenerating output. It\u2019s essential for training and testing models like machine translation or \nsummarization.\n\u2022 Unknown Token (<unk>): This is a fallback mechanism to handle out-of-vocabulary (OOV) words, \npreventing the model from crashing or producing nonsense if it encounters an unseen word \nduring inference.\n Next Step:\nNow that you\u2019ve set up your dataset and special tokens, you\u2019ll likely need to build a vocabulary and \nconvert these tokens into numerical IDs for training the model. Let me know if you want me to walk you \nthrough that process, or if you need help with any other part of the code!  \n What Does This Function Do?\nThis function builds a vocabulary from a list of tokenized sentences. The vocabulary maps each unique \ntoken (word) to a unique index. Additionally, it ensures that the special tokens (like <pad>, <sos>, etc.) \nare included in the vocabulary.\n   transformer Page 7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3048529c-f4aa-4266-bd70-ef21f4f9bc47": {"__data__": {"id_": "3048529c-f4aa-4266-bd70-ef21f4f9bc47", "embedding": null, "metadata": {"page_label": "8", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f968278-465f-4073-9468-bc9beb3839bb", "node_type": "4", "metadata": {"page_label": "8", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "bd3f74c106837d1eaa58ca96adc19b31f7a7f41dc2130de44414128004b68b47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "are included in the vocabulary.\nHere\u2019s the code you shared:\ndef create_vocab(tokenized_sentences, special_tokens):\n    vocab = {token: idx for idx, token in enumerate(special_tokens)}  # Step 1\n    \n    for sentence in tokenized_sentences:  # Step 2\n        for token in sentence:\n            if token not in vocab:  # Step 3\n                vocab[token] = len(vocab)  # Step 4\n    return vocab\nStep-by-Step Explanation:\n1. Initialize Vocabulary with Special Tokens:\nvocab = {token: idx for idx, token in enumerate(special_tokens)}\n\u25cb This initializes the vocabulary with the special tokens that you defined earlier (<pad>, <sos>, \n<eos>, <unk>).\n\u25cb enumerate(special_tokens) gives each token an index starting from 0. The special tokens are \nassigned indices in the order in which they appear in the special_tokens list.\nFor example, if special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>'], the initial vocabulary \nwould look like this:\nvocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n2. Iterate Over the Tokenized Sentences:\nfor sentence in tokenized_sentences:\n    for token in sentence:\n\u25cb The function loops through each sentence in the tokenized_sentences list, where each \nsentence is a list of tokens (words).\n\u25cb For each sentence, it iterates over the individual tokens (words).\n3. Add New Tokens to Vocabulary:\nif token not in vocab:\n    vocab[token] = len(vocab)\n\u25cb This checks if a token is not already in the vocabulary. If it's a new token (that wasn\u2019t in the \noriginal list of special tokens), it assigns the token a unique index.\n\u25cb The index is the current length of the vocabulary, ensuring that each new token gets a \nunique ID that\u2019s one greater than the previous token\u2019s index.\nExample:\nLet\u2019s say you have the following tokenized sentence:\ntokenized_sentences = [['hello', 'world'], ['how', 'are', 'you']]\nInitially, the vocabulary is:\nvocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\nAfter processing all the sentences, the vocabulary will become:\nvocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, 'hello': 4, 'world': 5, 'how': 6, 'are': 7, \n'you': 8}\n4. Return the Vocabulary:\nreturn vocab\n\u25cb Finally, the function returns the completed vocabulary, which contains both the special \ntokens and any additional tokens found in the sentences.\n Why Is This Function Important?\n\u2022 Mapping Words to Indices: Neural networks work with numerical data, so you need to convert \nwords into numbers. This vocabulary serves as a lookup table for converting tokens into indices \nand vice versa.\n   transformer Page 8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92c049e4-0b98-4564-baf9-c63d186125b3": {"__data__": {"id_": "92c049e4-0b98-4564-baf9-c63d186125b3", "embedding": null, "metadata": {"page_label": "9", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8288ff90-f499-40b7-bfd9-55647dabd13e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "eccd7fd8e4fa8306db5393a98ddc81952f7405f8ef8930f5f82c9110200a4393", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and vice versa.\n\u2022 Handling Special Tokens: Special tokens like <pad>, <sos>, etc., are crucial for sequence \nprocessing. Padding ensures sequences are of equal length, and the start and end tokens help \nwith generation tasks.\n\u2022 Extending Vocabulary: The function also allows you to dynamically add any new words that \nappear in the training data to the vocabulary. This is useful for models that need to handle out-of-\nvocabulary words.\n Next Step:\nOnce you have this vocabulary, you'll need to convert both the source and target sentences into \nsequences of indices (numerical representation) before passing them to the model. Let me know if you \nwant to move forward with that part or if you have any more questions!  \n What is Positional Encoding?\nTransformers don't have recurrence (like RNNs) or any notion of sequence order. To help them \nunderstand the order of words in a sequence, we add positional encodings to the input embeddings.\nEach position in the sequence gets a unique sinusoidal (sine & cosine) encoding.\n Step-by-Step Breakdown\nHere's the code you shared:\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=1000):\n        super().__init__()\n\u2022 This is a PyTorch module (nn.Module), meaning it can be used like a layer in a neural network.\n\u2022 d_model: The embedding size (how many features per token).\n\u2022 max_len: Maximum length of input sequences. Default is 1000, meaning we assume a sentence \ncan be at most 1000 tokens long.\n      t     o   o  l E  od      t  x\npe = torch.zeros(max_len, d_model)\n\u2022 Creates a max_len \u00d7 d_model matrix filled with zeros.\n\u2022 Each row represents a position (1st word, 2nd word, etc.).\n\u2022 Each column represents a dimension of the embedding.\n        t   o   o  I d    \nposition = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n\u2022 torch.arange(0, max_len) creates a vector of positions: [0, 1, 2, ..., max_len-1]\n\u2022 .unsqueeze(1) changes its shape from [max_len] \u2192 [ma _len, 1], making it a column vector.\nExample for max_len=5:\nposition = [[0],  \n            [1],  \n            [2],  \n            [3],  \n            [4]]\n   om  t          y    l       m\ndiv_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\u2022 This calculates different frequency scales for sine and cosine functions.\n\u2022 The 10000.0 factor helps distribute values smoothly.\n\u2022 The arange(0, d_model, 2) selects every even index (for sine) and every odd index (for cosine).\n   transformer Page 9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2476, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8476e1b3-b9cc-427f-bd26-dae9013263d4": {"__data__": {"id_": "8476e1b3-b9cc-427f-bd26-dae9013263d4", "embedding": null, "metadata": {"page_label": "10", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6ca99dcc-f0c1-49d3-8aeb-180b877f2a00", "node_type": "4", "metadata": {"page_label": "10", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "a5f7008900f75e913b2416e77ece6b1481a3cb29e70b056248e5c1e8511d9dd0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 The arange(0, d_model, 2) selects every even index (for sine) and every odd index (for cosine).\nExample when d_model=6:\n[0, 2, 4]  \u2192 for sine\n[1, 3, 5]  \u2192 for cosine\nEach position gets different sinusoidal values based on these scales.\n     ly         o    \npe[:, 0::2] = torch.sin(position * div_term)  # Even indices \u2192 sine\npe[:, 1::2] = torch.cos(position * div_term)  # Odd indices \u2192 cosine\n\u2022 Even columns (0, 2, 4, \u2026) get sine values.\n\u2022 Odd columns (1, 3, 5, \u2026) get cosine values.\n\u2022 This ensures different frequencies for different dimensions, giving a rich encoding.\nExample output for first 3 positions (d_model=6):\nPosition 0: [ 0.000,  1.000,  0.000,  1.000,  0.000,  1.000]\nPosition 1: [ 0.841,  0.540,  0.909,  0.416,  0.951,  0.309]\nPosition 2: [ 0.909, -0.416,  0.755, -0.655,  0.588, -0.809]\n\u2022 Notice how sine values oscillate between -1 and 1 as the position increases.\n     h      d  to           \npe = pe.unsqueeze(0).transpose(0, 1)\n\u2022 .unsqueeze(0): Adds a batch dimension \u2192 [1, ma _len, d_model]\n\u2022 .transpose(0, 1): Swaps first two dimensions \u2192 [ma _len, 1, d_model] \n\u25cb This helps in adding positional encoding across batches.\nself.register_buffer('pe', pe)\n\u2022 register_buffer stores pe inside the model, but it won't be updated during training (since it's not \na learnable parameter).\n   o    d     \ndef forward(self, x):\n    return x + self.pe[:x.size(0), :]\n\u2022 Takes x (word embeddings) as input.\n\u2022 Adds the positional encoding (pe[:x.size(0), :]) to it.\n\u2022 The x.size(0) ensures we only take the first N positions corresponding to the input length.\n Why Use Sin & Cos Functions?\n1. Periodic Nature: Words at similar positions get similar encodings, helping the model learn \nposition-related patterns.\n2. Unique Representation: Each position has a distinct encoding due to different sine & cosine \nfrequencies.\n3. Extrapolation: Unlike learned embeddings, sinusoidal encodings can generalize to unseen lengths.\n Summary\n\u2022 This adds positional information to word embeddings in a Transformer model.\n\u2022 Uses sinusoidal encodings (sine for even, cosine for odd).\n\u2022 Stored as a buffer so it doesn't update during training.\n\u2022 During forward(), adds positional encodings to input embeddings.\n What's Next?\n   transformer Page 10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2242, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d844248f-f515-4da4-94e1-26e939335820": {"__data__": {"id_": "d844248f-f515-4da4-94e1-26e939335820", "embedding": null, "metadata": {"page_label": "11", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "772b9db4-473c-490d-b7bd-8dd33cf61f11", "node_type": "4", "metadata": {"page_label": "11", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "4c59b1bf54bc564ead8ee14a7394982957d60638144c129167e9d9009472463f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What's Next?\nThis is a key part of Transformer models. If you're building an encoder-decoder model, next you'll likely \ndefine the embedding layer and self-attention mechanism. Let me know if you need help with that!  \n Example: Understanding Positional Encoding\nLet's manually run the PositionalEncoding class for a small example and see how it modifies an \nembedding.\nStep 1: Define Positional Encoding Class (Same as Before)\nimport torch\nimport math\nimport torch.nn as nn\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=10):  # Keeping max_len small for easy understanding\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\nposition = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # Shape: [max_len, 1]\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\npe[:, 0::2] = torch.sin(position * div_term)  # Apply sin to even indices\n        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cos to odd indices\npe = pe.unsqueeze(0)  # Shape becomes [1, max_len, d_model] (for batch compatibility)\n        self.register_buffer('pe', pe)\ndef forward(self, x):\n        return x + self.pe[:x.size(1), :]\nStep 2: Create a Dummy Input\nLet\u2019s assume we have a sentence with 5 words, and each word is represented by a 4-dimensional \nembedding (d_model=4).\n# Define embedding size\nd_model = 4\nmax_len = 10  # Maximum sentence length\n# Create Positional Encoding Layer\npos_encoder = PositionalEncoding(d_model, max_len)\n# Create a dummy embedding for a 5-word sentence (batch size = 1, seq_len = 5, d_model = 4)\ndummy_embedding = torch.zeros(1, 5, d_model)  # Shape: [batch, seq_len, d_model]\n# Apply positional encoding\nencoded_embedding = pos_encoder(dummy_embedding)\nprint(encoded_embedding)\n Step 3: Understanding the Output\nBefore Positional Encoding (Original dummy_embedding):\n[[[0.0000, 0.0000, 0.0000, 0.0000],  # Word 1\n  [0.0000, 0.0000, 0.0000, 0.0000],  # Word 2\n  [0.0000, 0.0000, 0.0000, 0.0000],  # Word 3\n  [0.0000, 0.0000, 0.0000, 0.0000],  # Word 4\n  [0.0000, 0.0000, 0.0000, 0.0000]]] # Word 5\nSince it was initialized with all zeros, each word starts with no unique information.\nAfter Applying Positional Encoding:\n[[[ 0.0000,  1.0000,  0.0000,  1.0000],  # Position 0\n  [ 0.8415,  0.5403,  0.0909,  0.9959],  # Position 1\n   transformer Page 11", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 2365, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1558bd5a-19c5-4ce9-8a51-a391588ddfcd": {"__data__": {"id_": "1558bd5a-19c5-4ce9-8a51-a391588ddfcd", "embedding": null, "metadata": {"page_label": "12", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a2cda1d-1a27-48a1-ba21-2be43b90da91", "node_type": "4", "metadata": {"page_label": "12", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "7578a650cda500fc9655a78a3682e9c96a44e826297b431753be4d20c2209092", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[ 0.8415,  0.5403,  0.0909,  0.9959],  # Position 1\n  [ 0.9093, -0.4161,  0.1818,  0.9839],  # Position 2\n  [ 0.1411, -0.9900,  0.2727,  0.9640],  # Position 3\n  [-0.7568, -0.6536,  0.3636,  0.9361]]] # Position 4\n Wh t\u2019  H         H   ?\n\u2022 The values are no longer zeros; each word in the sequence now has a unique positional encoding.\n\u2022 Even indices (0, 2) \u2192 Sinusoidal values (sin function).\n\u2022 Odd indices (1, 3) \u2192 Cosine values (cos function).\n\u2022 Patterns emerge in the position encodings, which the Transformer uses to understand word \norder.\n Summary\n1. Before: All embeddings were zero, meaning no positional information.\n2. After: Each word got a unique position-based modification, allowing the Transformer to \ndifferentiate between positions.\n3. Now, when these encoded embeddings go into the self-attention mechanism, the model \nunderstands their relative positions without recurrence.\n Next Steps\nNow that we have positional encodings, the next steps in a Transformer would be: \u2705Word \nEmbeddings (instead of zero vectors).\n\u2705Self-Attention Mechanism.\n\u2705Feed-Forward Network.\n Code for forward() Function\ndef forward(self, x):\n    return x + self.pe[:x.size(0), :]\n Breakdown Step-by-Step\n  x    th  I   t Em  dd   \ndef forward(self, x):\n\u2022 x yaha ek word embedding tensor hoga.\n\u2022 Shape: [seq_len, batch_size, d_model] \n\u25cb seq_len = Kitne words hain sequence me\n\u25cb batch_size = Kitne sentences ek saath pass ho rahe hain\n\u25cb d_model = Embedding size (jitne features per word)\nFor example, agar ek batch me 3 sentences hain, har ek 5 words ka aur embedding size 4 hai, to x ka \nshape hoga:\nx.shape = [5, 3, 4]\nMtlb:\n\u2022 5 words per sentence\n\u2022 3 sentences ek saath\n\u2022 4-dimensional embeddings per word\n   dd  o   o  l E  od   \nreturn x + self.pe[:x.size(0), :]\n\u2022 self.pe positional encoding matrix hai (size [max_len, 1, d_model]).\n\u2022 x.size(0) mtlb jitna x ka sequence length hoga utna hi pe ka part select hoga.\n   transformer Page 12", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 1934, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc731157-a527-4712-be13-e352c3567a31": {"__data__": {"id_": "fc731157-a527-4712-be13-e352c3567a31", "embedding": null, "metadata": {"page_label": "13", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "96b53d13-6ac1-40da-aa24-1f8afe0ecfcb", "node_type": "4", "metadata": {"page_label": "13", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "97fade57a7fb1b55716855128b72026cf17a03324450f0da830bd8c7dfc258d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 x.size(0) mtlb jitna x ka sequence length hoga utna hi pe ka part select hoga.\n\u2022 Broadcasting hoti hai, yani har batch ke liye positional encoding automatically apply ho jata hai.\n Example Run\nAgar x ek random embedding ho:\nx = torch.tensor([\n    [ [1.0, 2.0, 3.0, 4.0] ],  # Word 1\n    [ [5.0, 6.0, 7.0, 8.0] ],  # Word 2\n    [ [9.0, 10.0, 11.0, 12.0] ] # Word 3\n])  # Shape: [3, 1, 4] (3 words, batch=1, 4 features per word)\nAur positional encoding pe kuch aisa ho:\npe = [\n    [ 0.0,  1.0,  0.0,  1.0],  # Position 1\n    [ 0.8,  0.5,  0.1,  0.9],  # Position 2\n    [ 0.9, -0.4,  0.2,  0.8]   # Position 3\n]  # Shape: [3, 1, 4]\nTo final output hoga:\nx + pe = [\n    [ 1.0+0.0,  2.0+1.0,  3.0+0.0,  4.0+1.0 ],  # Word 1\n    [ 5.0+0.8,  6.0+0.5,  7.0+0.1,  8.0+0.9 ],  # Word 2\n    [ 9.0+0.9, 10.0-0.4, 11.0+0.2, 12.0+0.8 ]   # Word 3\n]\nFinal Result:\n[\n    [1.0, 3.0, 3.0, 5.0],\n    [5.8, 6.5, 7.1, 8.9],\n    [9.9, 9.6, 11.2, 12.8]\n]\n Summary\n\u2705x hota hai word embeddings.\n\u2705self.pe hota hai precomputed positional encodings.\n\u2705x + self.pe[:x.size(0), :] har word embedding me positional encoding add kar deta hai.\n\u2705Ye model ko word ka position samjhne me madad karta hai bina recurrence ke!\n Multi-Head Attention Overview\n Attention ka kaam kya hai?\n Ye decides karta hai ki input ke kaunse words important hai while processing a sentence.\n Multi-Head Attention ka kya fayda hai?\n Ek hi input me multiple attention mechanisms parallelly kaam karte hain to capture different \nrelationships.\n   o  t   to        t      I    l    o \nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\nself.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads  # Each head ka dimension\n   transformer Page 13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1864, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aed5196b-e68a-47f4-b9a8-f59785ccce55": {"__data__": {"id_": "aed5196b-e68a-47f4-b9a8-f59785ccce55", "embedding": null, "metadata": {"page_label": "14", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b78ea85e-05c7-4b25-86fb-89793ca403c8", "node_type": "4", "metadata": {"page_label": "14", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "33304076220aa66c56aa44ab83219e1dad2977e438620385c4b1356e5c68d7f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "self.d_k = d_model // num_heads  # Each head ka dimension\nself.W_q = nn.Linear(d_model, d_model)  # Query matrix\n        self.W_k = nn.Linear(d_model, d_model)  # Key matrix\n        self.W_v = nn.Linear(d_model, d_model)  # Value matrix\n        self.W_o = nn.Linear(d_model, d_model)  # Final output linear layer\n Explanation\n1. d_model \u2192 Input embedding ka size (usually 512 or 768).\n2. num_heads \u2192 Kitne attention heads chahiye (e.g. 8 heads).\n3. Each head ke liye dimension:\nself.d_k = d_model // num_heads  # e.g. 512/8 = 64\nHar head ka size d_k hota hai (64 in this case).\n4. W_q, W_k, W_v \u2192 Ye Query (Q), Key (K), aur Value (V) vectors generate karte hain.\n\u25cb Query (Q) \u2192 Kis word ko dhyan dena hai?\n\u25cb Key (K) \u2192 Kaunse words important hai?\n\u25cb Value (V) \u2192 Actual information jo pass hogi.\n     l d dot   od  t      o       om  t       o \ndef scaled_dot_product_attention(self, Q, K, V, mask=None):\n    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n    if mask is not None:\n        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)  # Ignore padding tokens\nattn_probs = torch.softmax(attn_scores, dim=-1)  # Normalize scores\n    output = torch.matmul(attn_probs, V)  # Multiply with Values\n    return output\n Explanation\n1. Dot Product between Q and K\nattn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\u25cb Query aur Key ka dot product le rahe hain.\n\u25cb Scaling factor 1/sqrt(d_k) ensure karta hai ki values stable rahe (NaN na aaye).\n2. Apply Mask (if needed)\nif mask is not None:\n    attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n\u25cb Padding ya future words ignore karne ke liye mask use hota hai.\n3. Softmax Normalization\nattn_probs = torch.softmax(attn_scores, dim=-1)\n\u25cb Ye probability distribution generate karta hai.\n4. Multiply with Values\noutput = torch.matmul(attn_probs, V)\n\u25cb Important words ki values ko weightage milta hai.\n  fo    d      om l t   lo \ndef forward(self, Q, K, V, mask=None):\n    batch_size = Q.size(0)\nQ = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n    K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n    V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\noutput = self.scaled_dot_product_attention(Q, K, V, mask)\noutput = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n    return self.W_o(output)\n Explanation\n   transformer Page 14", "mimetype": "text/plain", "start_char_idx": 8, "end_char_idx": 2443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e6887d47-75c4-4050-8d98-53fe3e232c13": {"__data__": {"id_": "e6887d47-75c4-4050-8d98-53fe3e232c13", "embedding": null, "metadata": {"page_label": "15", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "28e732bb-d22b-47a5-b332-a8b6634faf14", "node_type": "4", "metadata": {"page_label": "15", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "0a599e91565be94d04b5199388e6ad47f712b267d2b0430f3f47b40538dede0c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Explanation\n1. Q, K, V Generate\nQ = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n\u25cb W_q(Q): Query generate kiya\n\u25cb .view(...): Multi-heads me reshape kiya\n\u25cb .transpose(1, 2): Batch aur heads swap kiya (shape [batch, heads, seq_len, d_k] ban gaya)\n2. Apply Scaled Dot Product Attention\noutput = self.scaled_dot_product_attention(Q, K, V, mask)\n\u25cb Ye attention weights calculate karke values multiply karta hai.\n3. Concatenate Multi-Heads\noutput = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n\u25cb Heads ko combine karke wapas ek single vector banaya.\n\u25cb .contiguous() memory ko optimize karta hai.\n4. Final Linear Layer\nreturn self.W_o(output)\n\u25cb Ye final output layer hai jo next transformer block me jayega.\n Example - Running Multi-Head Attention\n# Define Attention layer\nmha = MultiHeadAttention(d_model=8, num_heads=2)\n# Dummy Input (batch=1, seq_len=4, d_model=8)\nx = torch.rand(1, 4, 8)\n# Run forward pass\noutput = mha(x, x, x)\nprint(output.shape)  # Output: [1, 4, 8]\n What Happens Here?\n1. Q, K, V banega (embedding ke liye projection layer lagegi).\n2. Multi-head attention compute hoga (scaled dot-product attention).\n3. Concatenation hoke final output milega (size d_model ka hi hoga).\n Summary\nStep Explanation\n1. Q, K, V Generate Input embeddings ko query, key, aur value vectors me map \nkiya\n2. Dot-Product Attention Q aur K ka dot product leke attention scores nikala\n3. Softmax & Value Multiplication Softmax apply kiya aur V ke saath multiply kiya\n4. Multi-Heads Apply Multiple heads ko parallelly apply kiya\n5. Concatenation & Output Linear \nLayer\nHeads ka output combine karke linear layer se pass kiya\n Final Thoughts\n\u2705Multi-Head Attention se model ek hi sentence me multiple relationships detect kar sakta hai!\n\u2705Ye Transformer ka most important layer hai, jo self-attention ko enhance karta hai.\n\u2705Iske bina Transformer properly kaam nahi karega.\n Example Sentence\n   transformer Page 15", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 1955, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7a58867c-9dfd-4b7b-ac50-14bc9b1fdabc": {"__data__": {"id_": "7a58867c-9dfd-4b7b-ac50-14bc9b1fdabc", "embedding": null, "metadata": {"page_label": "16", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7020f88-8394-467b-9b6a-5c611ef616e2", "node_type": "4", "metadata": {"page_label": "16", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "dd3a7f9f5e95021b36d17f2706ad4a0ed36b38e0bd01ca0e6f17293ef2493eb1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Example Sentence\n Suppose, we have this English sentence:\n\"The cat sat on the mat.\"\n                                              \nHar word ek vector ban jayega (embedding size = 8).\nMaan lo, word embeddings kuch aise hain:\nWord Embedding (Size 8)\n\"The\" [0.1, 0.3, 0.5, ...]\n\"cat\" [0.2, 0.4, 0.6, ...]\n\"sat\" [0.7, 0.1, 0.9, ...]\n\"on\" [0.3, 0.6, 0.8, ...]\n\"the\" [0.1, 0.3, 0.5, ...]\n\"mat\" [0.4, 0.7, 0.2, ...]\nIska tensor representation hoga:\nimport torch\n# Batch = 1, Seq_Len = 6, d_model = 8\nx = torch.rand(1, 6, 8)  # (Random embeddings)\n      y        y        l         t            t \nimport torch.nn as nn\nmha = MultiHeadAttention(d_model=8, num_heads=2)  # 2 heads\nQ = mha.W_q(x)  # Query\nK = mha.W_k(x)  # Key\nV = mha.W_v(x)  # Value\n  Wh t h      ?\n\u2022 Query (Q): Kis word ko focus karna hai?\n\u2022 Key (K): Kaunse words important hai?\n\u2022 Value (V): Jo actual information pass hogi.\n   ot   od  t      o     m l   ty   l  l  o  \nattn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (8 ** 0.5)  \n  Ex m l    t  t:\nWord \"The\" \"cat\" \"sat\" \"on\" \"the\" \"mat\"\nThe 1.0 0.7 0.5 0.3 1.0 0.4\ncat 0.7 1.0 0.8 0.4 0.7 0.5\nsat 0.5 0.8 1.0 0.6 0.5 0.7\non 0.3 0.4 0.6 1.0 0.3 0.8\nthe 1.0 0.7 0.5 0.3 1.0 0.4\nmat 0.4 0.5 0.7 0.8 0.4 1.0\n High Attention Values   S      R la     h  \n\u2022 \"cat\" ne \"sat\" par zyada attention diya (0.8) \u2705\n\u2022 \"on\" ne \"mat\" par zyada attention diya (0.8) \u2705\n   o m x   W   ht d   m      l      o    t  t \n   transformer Page 16", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 1436, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25439d0e-a06a-4b07-83e2-944bdca8690e": {"__data__": {"id_": "25439d0e-a06a-4b07-83e2-944bdca8690e", "embedding": null, "metadata": {"page_label": "17", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "adbfeec2-5a5f-4ff4-9aef-1474496dc35e", "node_type": "4", "metadata": {"page_label": "17", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "c8019f5d2129d06ef3654dc2659f8449d1e6285fb5338689a141046ba0ed9bef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "o m x   W   ht d   m      l      o    t  t \nimport torch.nn.functional as F\nattn_probs = F.softmax(attn_scores, dim=-1)  # Normalize\noutput = torch.matmul(attn_probs, V)\n  Ex m l   o m x   t  t:\nWord \"The\" \"cat\" \"sat\" \"on\" \"the\" \"mat\"\nThe 0.25 0.20 0.15 0.10 0.25 0.05\ncat 0.20 0.30 0.25 0.10 0.10 0.05\nsat 0.15 0.25 0.30 0.15 0.05 0.10\non 0.10 0.10 0.15 0.35 0.10 0.20\nthe 0.25 0.20 0.15 0.10 0.25 0.05\nmat 0.05 0.05 0.10 0.20 0.05 0.55\nKaise interpret kare?\n\u2022 \"cat\" ka focus zyada hai \"sat\" (0.25) aur khud par (0.30)\n\u2022 \"on\" ka focus \"mat\" (0.20) aur khud par (0.35)\n  Yahi reason hai ki Self-Attention sentence me relationships ko samajhne me madad karta hai!\n    l  H  d      o       ll l H  d  \nEk attention multiple heads me parallelly kaam karta hai.\n 1st Head \u2192 Focuses on subject-object relations\n 2nd Head \u2192 Focuses on positional dependencies\noutput = output.view(1, 6, 8)  # Merge heads\nfinal_output = mha.W_o(output)  # Final linear layer\n Final Output bhi 8-dimension ka hota hai (same as input size).\n Summary\nStep Explanation\n1. Create Q, K, V Words ko Query, Key, aur Value vectors me convert kiya\n2. Dot-Product Attention Words ka similarity score calculate kiya\n3. Softmax Normalize Scores ko probabilities me convert kiya\n4. Weighted Sum with Values Important words ka final output nikala\n5. Multi-Head Attention Multiple perspectives add kiye\n Real-World Use Case\n   a h      a  la      a  l      l  h       a  \nEnglish Input: \"The cat sat on the mat.\"\nGerman Output: \"Die Katze sa\u00df auf der Matte.\"\nMulti-Head Attention ensure karega ki: \u2705\"The\" ka attention \"Die\" par ho\n\u2705\"cat\" ka attention \"Katze\" par ho\n\u2705\"sat\" ka a         \" a\u00df\"  a  h \n Without Attention, ye alignment problematic ho sakti thi.  \n Final Thoughts\n\u2705Multi-Head Attention Transformer ka core hai!\n   transformer Page 17", "mimetype": "text/plain", "start_char_idx": 3, "end_char_idx": 1808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13e9062c-84eb-4aba-bde6-87032e8411c3": {"__data__": {"id_": "13e9062c-84eb-4aba-bde6-87032e8411c3", "embedding": null, "metadata": {"page_label": "18", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "10a790e5-5165-4b89-bd06-8e554e694b9d", "node_type": "4", "metadata": {"page_label": "18", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "77771865424f264dd6d23a2a50232562d02be2bcac6127856bb2b3dcb8730287", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2705Multi-Head Attention Transformer ka core hai!\n\u2705Words ke relationships detect karne me help karta hai.\n\u2705Ye Self-Attention ko aur powerful banata hai.\nClass Overview:\nThe feedforward network is an important component of the Transformer architecture. It is applied \nindependently to each position in the sequence (hence \"position-wise\"). This means that the same \nfeedforward neural network is applied to each token in the sequence, one at a time, without any \ninteraction between them.\nNow, let's dive into the code:\nclass PositionwiseFeedforward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)  # First linear layer\n        self.fc2 = nn.Linear(d_ff, d_model)  # Second linear layer\n        self.relu = nn.ReLU()  # ReLU activation function\ndef forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))  # Apply two linear layers with ReLU activation\nExplanation of Code:\n1. Constructor (__init__ method):\n\u25cb d_model: This is the dimensionality of the input (and output) of the model. In the \nTransformer, this corresponds to the size of the input vector for each token.\n\u25cb d_ff: This is the dimensionality of the intermediate layer in the feedforward network, i.e., \nhow many neurons there will be in the hidden layer between the two linear \ntransformations.\nThe feedforward network consists of two linear layers:\n\u25cb self.fc1: The first linear transformation takes the input of dimension d_model and \ntransforms it to the higher dimensional space of d_ff.\n\u25cb self.fc2: The second linear transformation projects the result from d_ff back to d_model.\n\u25cb self.relu: A ReLU activation function is applied between the two linear transformations. This \nintroduces non-linearity into the model, which is crucial for learning complex patterns.\n1. Forward Method:\n\u25cb The forward method defines how the input tensor x (which is passed through this layer) is \nprocessed.\n\u25cb Flow:\n1. The input x passes through the first linear layer fc1, which transforms the input from \nd_model dimensions to d_ff dimensions.\n2. The output of fc1 is passed through the ReLU activation function (self.relu). This step \nadds non-linearity to the transformation.\n3. Then, the activated output is passed through the second linear layer fc2, which brings \nit back to the original dimension d_model.\nSo, this feedforward layer takes the input, applies a transformation, adds non-\nlinearity, and transforms it again back to the original dimension.\nRole of Positionwise Feedforward in Transformer:\nIn the Transformer model:\n\u2022 This position-wise feedforward layer is applied independently to each token (or position) in the \nsequence.\n\u2022 The reason it's called \"position-wise\" is that each token is treated separately and is transformed \nindependently of the others, unlike the attention layers, which allow tokens to attend to each \n   transformer Page 18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2895, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c43d09d4-188a-436f-ac94-8d6ddef65372": {"__data__": {"id_": "c43d09d4-188a-436f-ac94-8d6ddef65372", "embedding": null, "metadata": {"page_label": "19", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4611a9e2-15bf-402e-9373-af165e09db4c", "node_type": "4", "metadata": {"page_label": "19", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "d488e62ef2636cb52e545a650b2c51900841ea7d7c503e381bcc23da6e895a80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "independently of the others, unlike the attention layers, which allow tokens to attend to each \nother.\nThis operation is repeated for each token in the sequence after the multi-head attention block. This \nallows each token to have its representation refined.\nSummary:\n\u2022 PositionwiseFeedforward is a simple two-layer feedforward network used in the Transformer.\n\u2022 It processes each token's representation independently, applying a non-linear transformation.\n\u2022 It consists of two linear layers with a ReLU activation function between them.\nLet me know if you need more details or examples on this!  \nThis class EncoderLayer represents one layer of the encoder in the Transformer model. The encoder \nlayer is the building block of the transformer encoder, and it consists of two main components: a multi-\nhead self-attention layer and a position-wise feedforward layer.\nLet's break down the code step by step.\nClass Breakdown:\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Multi-head attention layer\n        self.feed_forward = PositionwiseFeedforward(d_model, d_ff)  # Position-wise feedforward layer\n        self.norm1 = nn.LayerNorm(d_model)  # Layer normalization for the first component (attention)\n        self.norm2 = nn.LayerNorm(d_model)  # Layer normalization for the second component \n(feedforward)\n        self.dropout = nn.Dropout(dropout)  # Dropout to prevent overfitting\ndef forward(self, x, mask):\n        # Step 1: Self-attention\n        attn_output = self.self_attn(x, x, x, mask)  # Query, Key, Value all come from 'x' (self-attention)\n        x = self.norm1(x + self.dropout(attn_output))  # Apply residual connection, dropout, and \nnormalization\n# Step 2: Feedforward layer\n        ff_output = self.feed_forward(x)  # Apply position-wise feedforward network\n        x = self.norm2(x + self.dropout(ff_output))  # Apply residual connection, dropout, and normalization\nreturn x\nExplanation of Components:\n1. self.self_attn = MultiHeadAttention(d_model, num_heads)\n\u25cb This initializes the multi-head attention layer. It takes the input x (which represents the \ntokens' embeddings) as the query, key, and value. The self-attention mechanism allows \neach token to attend to all other tokens in the sequence.\n2. self.feed_forward = PositionwiseFeedforward(d_model, d_ff)\n\u25cb This initializes the position-wise feedforward layer. It is applied after the attention \nmechanism, independently to each token, as explained earlier.\n3. self.norm1 = nn.LayerNorm(d_model)\n\u25cb Layer normalization is applied after the self-attention block. Layer normalization helps \nstabilize and speed up training by normalizing the activations.\n\u25cb It ensures that the output of the attention layer has zero mean and unit variance across \neach batch.\n4. self.norm2 = nn.LayerNorm(d_model)\n\u25cb Similarly, layer normalization is applied after the feedforward layer for stability.\n5. self.dropout = nn.Dropout(dropout)\nDropout is a regularization technique applied to the outputs of the attention and \n   transformer Page 19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cc8f10f8-93c8-45b1-8e96-909ea9271f51": {"__data__": {"id_": "cc8f10f8-93c8-45b1-8e96-909ea9271f51", "embedding": null, "metadata": {"page_label": "20", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "23cc4249-e09f-49b7-9dc6-02d5669dfc39", "node_type": "4", "metadata": {"page_label": "20", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "844f0c8da16514577f3bc43ce0fdbd2ed8ece12d6a1c7e73bd52a4af431a7ca8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u25cb Dropout is a regularization technique applied to the outputs of the attention and \nfeedforward layers to prevent overfitting during training. dropout is the probability of \nsetting a unit to zero during training.\nForward Method:\nThe forward method defines how the data flows through the encoder layer. Here's how it works:\n1. Self-Attention:\n\u25cb The input x passes through the multi-head attention mechanism (self.self_attn(x, x, x, \nmask)).\n\u25cb The mask can be used to prevent certain tokens from attending to others (e.g., padding \ntokens or future tokens).\n\u25cb The result of the attention (attn_output) is then added to the original input x as a residual \nconnection. This allows the model to learn both the attention output and the original input \nin parallel.\n\u25cb Dropout is applied to the attention output, and then layer normalization is applied on top \nof the sum (x + attn_output).\n2. Feedforward Layer:\n\u25cb The output from the attention mechanism (after normalization) passes through the \nposition-wise feedforward layer (self.feed_forward(x)).\n\u25cb Again, a residual connection is applied by adding the original input x to the output of the \nfeedforward layer. This helps the network retain information from earlier stages.\n\u25cb Dropout is applied again, and layer normalization is done on the final output.\n3. Return:\n\u25cb The final output x after the attention and feedforward operations is returned, which is \npassed to the next layer of the encoder (or used for further processing, depending on the \narchitecture).\nSummary:\n\u2022 The EncoderLayer consists of two main components:\n1. Self-Attention: Helps the model focus on different parts of the input sequence.\n2. Feedforward Layer: Processes each token independently after attention.\n\u2022 Residual Connections: After each of the two operations, the input is added to the output (residual \nconnection). This helps with gradient flow during backpropagation and prevents \nvanishing/exploding gradients.\n\u2022 Layer Normalization: Normalizes the output after each operation (attention and feedforward) for \nstable training.\n\u2022 Dropout: Applied to prevent overfitting during training.\nThis architecture is repeated in the encoder stack, with each layer applying self-attention and \nfeedforward processing.\nThis class DecoderLayer implements a decoder layer for the Transformer model. The decoder in the \nTransformer consists of three main components:\n1. Self-attention mechanism (similar to the encoder but with a mask to prevent future tokens from \nattending to previous ones)\n2. Cross-attention mechanism (to attend to the encoder's output)\n3. Feedforward layer (applied independently to each position in the sequence)\nLet's break down the code step by step to understand how it works.\nClass Breakdown:\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Self-attention mechanism\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)  # Cross-attention mechanism\n   transformer Page 20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3064, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8972e66c-a028-4e86-829c-ec2c5bf17daf": {"__data__": {"id_": "8972e66c-a028-4e86-829c-ec2c5bf17daf", "embedding": null, "metadata": {"page_label": "21", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c34b811-bb5c-4b74-a7c8-5132e5f0fbd8", "node_type": "4", "metadata": {"page_label": "21", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "cba71ad46e755d65a5e2df3f62f3c0f14adea0ea7356b72ef29aa1c959624745", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "self.cross_attn = MultiHeadAttention(d_model, num_heads)  # Cross-attention mechanism\n        self.feed_forward = PositionwiseFeedforward(d_model, d_ff)  # Feedforward network\n        self.norm1 = nn.LayerNorm(d_model)  # Layer normalization after self-attention\n        self.norm2 = nn.LayerNorm(d_model)  # Layer normalization after cross-attention\n        self.norm3 = nn.LayerNorm(d_model)  # Layer normalization after feedforward\n        self.dropout = nn.Dropout(dropout)  # Dropout to prevent overfitting\ndef forward(self, x, enc_output, src_mask, trg_mask):\n        # Step 1: Self-attention (using the target sequence with masking)\n        attn_output = self.self_attn(x, x, x, trg_mask)  # Query, Key, Value = x (self-attention)\n        x = self.norm1(x + self.dropout(attn_output))  # Apply residual connection, dropout, and \nnormalization\n# Step 2: Cross-attention (using the encoder output with source masking)\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)  # Query = x, Key/Value = \nencoder output\n        x = self.norm2(x + self.dropout(attn_output))  # Apply residual connection, dropout, and \nnormalization\n# Step 3: Feedforward layer\n        ff_output = self.feed_forward(x)  # Apply position-wise feedforward network\n        x = self.norm3(x + self.dropout(ff_output))  # Apply residual connection, dropout, and normalization\nreturn x\nExplanation of Components:\n1. self.self_attn = MultiHeadAttention(d_model, num_heads):\n\u25cb This initializes the multi-head self-attention layer. It processes the target sequence (x) to \nallow the model to attend to all previous tokens in the sequence (during training) while \npreventing attending to future tokens (via masking).\n\u25cb This step ensures that during training, the model doesn't cheat by looking at future tokens.\n2. self.cross_attn = MultiHeadAttention(d_model, num_heads):\n\u25cb This is the cross-attention mechanism. Here, the model attends to the output of the \nencoder (enc_output) while processing the target sequence (x).\n\u25cb The query comes from the target sequence (x), while the key and value come from the \nencoder's output (enc_output).\n\u25cb This mechanism allows the decoder to focus on relevant parts of the input (source) \nsequence.\n3. self.feed_forward = PositionwiseFeedforward(d_model, d_ff):\n\u25cb This is the position-wise feedforward network, applied to each token independently, as \ndescribed in previous explanations.\n\u25cb This component is responsible for learning more complex representations after the \nattention steps.\n4. self.norm1, self.norm2, self.norm3:\n\u25cb Layer normalization is applied after each of the three steps: \n1. After self-attention (to normalize the result of self-attention)\n2. After cross-attention (to normalize the result of cross-attention)\n3. After the feedforward network (to normalize the output of the feedforward network)\n\u25cb This normalization helps stabilize training and allows for faster convergence.\n5. self.dropout:\n\u25cb Dropout is applied to prevent overfitting during training. It randomly sets some values to \nzero in the output, forcing the model to generalize better.\nForward Method:\nThe forward method defines the data flow through the decoder layer. Here's how it works:\n1. Self-Attention:\nThe input x (the target sequence) passes through the self-attention mechanism.\n   transformer Page 21", "mimetype": "text/plain", "start_char_idx": 8, "end_char_idx": 3333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "542e88c8-14d9-4d10-962b-9b56f7afec3c": {"__data__": {"id_": "542e88c8-14d9-4d10-962b-9b56f7afec3c", "embedding": null, "metadata": {"page_label": "22", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "523d422c-6f89-49f0-8548-2ceb6209177a", "node_type": "4", "metadata": {"page_label": "22", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "03cdf40c318fb8b12b4098108a43fbe2303669d90d58259859f48c284e480a40", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u25cb The input x (the target sequence) passes through the self-attention mechanism.\n\u25cb The trg_mask (target mask) is used to prevent attending to future tokens during training, \nensuring that the model predicts each token only using previous tokens (auto-regressive \nproperty).\n\u25cb After applying the attention, a residual connection is added (x + attn_output), followed by \ndropout and layer normalization.\n2. Cross-Attention:\n\u25cb The output of the self-attention mechanism (after normalization) is passed to the cross-\nattention mechanism.\n\u25cb Here, the encoder output (enc_output) is used as the key and value, while the target \nsequence (x) is used as the query.\n\u25cb This allows the decoder to focus on relevant parts of the source (input) sequence.\n\u25cb Again, a residual connection, followed by dropout and layer normalization, is applied.\n3. Feedforward Layer:\n\u25cb The output of the cross-attention mechanism (after normalization) passes through the \nposition-wise feedforward network.\n\u25cb This further refines the representations for each token in the sequence.\n\u25cb A final residual connection, followed by dropout and layer normalization, is applied.\n4. Return:\n\u25cb The final output x is returned, which represents the processed target sequence after \nattention and feedforward processing. This output is passed to the next decoder layer (or \nthe output layer if it's the last decoder layer).\nSummary:\n\u2022 Self-Attention: Allows the decoder to focus on relevant tokens in the target sequence while \nensuring that future tokens aren't attended to (via masking).\n\u2022 Cross-Attention: Allows the decoder to attend to relevant parts of the source sequence (from the \nencoder).\n\u2022 Feedforward Layer: Refines the representations of each token independently.\n\u2022 Residual Connections: Help with gradient flow and prevent vanishing/exploding gradients.\n\u2022 Layer Normalization: Stabilizes the training by normalizing the output at each stage.\n\u2022 Dropout: Prevents overfitting by randomly setting values to zero during training.\nThis architecture is repeated across multiple decoder layers to process and refine the target sequence's \nrepresentation. Each decoder layer refines the sequence based on both the target sequence (via self-\nattention) and the source sequence (via cross-attention).\nThis code defines the Transformer model, which is the complete architecture that combines the encoder \nand decoder layers with additional components such as embeddings, positional encoding, and the final \noutput layer.\nClass Breakdown:\n__init__ Method:\nThis method initializes all the components of the Transformer model, including embeddings, positional \nencoding, layers, and final output layer.\ndef __init__(self, src_vocab_size, trg_vocab_size, d_model, num_heads, num_layers, d_ff, \nmax_seq_length, dropout):\n    super().__init__()\n    \n    # Embeddings for source and target sequences\n    self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)  # Source sequence embedding\n    self.decoder_embedding = nn.Embedding(trg_vocab_size, d_model)  # Target sequence embedding\n    \n    # Positional encoding to add positional information to the embeddings\n    self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n   transformer Page 22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bc71aea8-4100-4845-bb25-59571125e140": {"__data__": {"id_": "bc71aea8-4100-4845-bb25-59571125e140", "embedding": null, "metadata": {"page_label": "23", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f22d37ed-ada0-4778-9e4d-4588c832f236", "node_type": "4", "metadata": {"page_label": "23", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "caf8f3d3af934ff565e9131c1068becfbc8327bd75f3bcea0362c51bed4fb900", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n    \n    # Stacks of encoder and decoder layers\n    self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in \nrange(num_layers)])\n    self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in \nrange(num_layers)])\n    \n    # Final output layer (to produce predictions for each token)\n    self.fc_out = nn.Linear(d_model, trg_vocab_size)\n    \n    # Dropout to prevent overfitting\n    self.dropout = nn.Dropout(dropout)\n    \n    # Scaling factor for the embeddings\n    self.scale = torch.sqrt(torch.FloatTensor([d_model]))\nExplanation of Each Component:\n1. encoder_embedding and decoder_embedding:\n\u25cb These are embedding layers for the source (src) and target (trg) sequences. These layers \ntransform input tokens (words) into dense vector representations of size d_model.\n2. positional_encoding:\n\u25cb Positional Encoding is added to the embeddings to incorporate the order of tokens in the \nsequence. Since the Transformer doesn\u2019t inherently have any sense of order, this encoding \nadds information about the position of each token in the sequence.\n3. encoder_layers and decoder_layers:\n\u25cb These are lists of encoder and decoder layers, which are instances of the EncoderLayer and \nDecoderLayer classes, respectively. The number of layers in the encoder and decoder is \ndetermined by num_layers.\n4. fc_out:\n\u25cb This is a fully connected layer that maps the output of the decoder to the target vocabulary \nsize (trg_vocab_size). It produces logits for each word in the vocabulary at each position in \nthe sequence.\n5. dropout:\n\u25cb A dropout layer is used for regularization, which randomly drops a proportion of neurons \nduring training to prevent overfitting.\n6. scale:\n\u25cb The embeddings are scaled by a factor of the square root of the model dimension \n(d_model). This scaling helps stabilize the training.\ngenerate_mask Method:\ndef generate_mask(self, src, trg):\n    # Mask for the source sequence (to ignore padding tokens in the source)\n    src_mask = (src != SRC_VOCAB[PAD_TOKEN]).unsqueeze(1).unsqueeze(2)\n    \n    # Mask for the target sequence (to ignore padding tokens in the target)\n    trg_mask = (trg != TRG_VOCAB[PAD_TOKEN]).unsqueeze(1).unsqueeze(3)\n    \n    # Create a \"no-peak\" mask for the target sequence (to prevent attending to future tokens in the \ndecoder)\n    seq_length = trg.shape[1]\n    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n    trg_mask = trg_mask & nopeak_mask  # Combine padding mask and no-peak mask\n    \n    return src_mask, trg_mask\n   transformer Page 23", "mimetype": "text/plain", "start_char_idx": 4, "end_char_idx": 2674, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0b825b6d-a0c3-4b11-9f93-c4a863d4a2ea": {"__data__": {"id_": "0b825b6d-a0c3-4b11-9f93-c4a863d4a2ea", "embedding": null, "metadata": {"page_label": "24", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "808a7ae9-6b1a-41ee-b194-63087b7fdcb7", "node_type": "4", "metadata": {"page_label": "24", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "c3da97857af5d02f06bc19a9a7663734f855f530cf009f0e0c7f8d912db0b7fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "return src_mask, trg_mask\n\u2022 src_mask: This mask ensures that padding tokens in the source sequence are ignored during \nattention computation in both the encoder and decoder.\n\u2022 trg_mask: This mask ensures that padding tokens in the target sequence are ignored during \nattention computation in the decoder.\n\u2022 nopeak_mask: This mask ensures that during training, the decoder can only attend to earlier \npositions and not future positions in the target sequence (causing the model to generate tokens in \nan auto-regressive manner). It's a key part of the causal masking in the decoder.\nforward Method:\ndef forward(self, src, trg):\n    src_mask, trg_mask = self.generate_mask(src, trg)\n    \n    # Embedding and positional encoding for the source and target sequences\n    src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src) * self.scale))\n    trg_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(trg) * self.scale))\n    \n    # Encoder forward pass\n    enc_output = src_embedded\n    for enc_layer in self.encoder_layers:\n        enc_output = enc_layer(enc_output, src_mask)\n    \n    # Decoder forward pass\n    dec_output = trg_embedded\n    for dec_layer in self.decoder_layers:\n        dec_output = dec_layer(dec_output, enc_output, src_mask, trg_mask)\n    \n    # Final output (logits for each token in the target sequence)\n    output = self.fc_out(dec_output)\n    return output\n1. Generate Masks:\n\u25cb The generate_mask function is called to create masks for the source and target sequences.\n2. Embedding and Positional Encoding:\n\u25cb The input src (source) and trg (target) sequences are first passed through their respective \nembedding layers and then multiplied by the scaling factor (scale).\n\u25cb Positional encoding is added to the embeddings to introduce information about the position \nof each token in the sequence.\n\u25cb Dropout is applied for regularization.\n3. Encoder Forward Pass:\n\u25cb The embedded source sequence is passed through the stack of encoder layers. The \nsrc_mask ensures that padding tokens are ignored during attention calculations.\n4. Decoder Forward Pass:\n\u25cb The embedded target sequence is passed through the stack of decoder layers.\n\u25cb Both the encoder output (enc_output) and the masks (src_mask and trg_mask) are passed \nto the decoder layers.\n5. Final Output:\n\u25cb The decoder output is passed through the final fully connected layer (fc_out) to produce \nthe logits for each token in the target vocabulary.\n\u25cb These logits are used to predict the next token in the sequence.\nSummary:\n\u2022 The Transformer model consists of embedding layers for both the source and target sequences, \npositional encoding, and a stack of encoder and decoder layers.\n\u2022 The generate_mask function creates the necessary masks to ignore padding and prevent the \n   transformer Page 24", "mimetype": "text/plain", "start_char_idx": 4, "end_char_idx": 2820, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8081dad5-b35a-4ea0-83d0-9c803016a5de": {"__data__": {"id_": "8081dad5-b35a-4ea0-83d0-9c803016a5de", "embedding": null, "metadata": {"page_label": "25", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8805232e-3503-418b-b19e-e81313da17ec", "node_type": "4", "metadata": {"page_label": "25", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "54839b74c85c7c7f91830df0984cfac47959640b7f5a9171793513473494bb77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 The generate_mask function creates the necessary masks to ignore padding and prevent the \nmodel from attending to future tokens during training.\n\u2022 The forward pass involves embedding the input sequences, passing them through the encoder and \ndecoder layers, and producing logits for the target sequence.\nThis Transformer model can be trained to perform tasks like machine translation, where the goal is to \ntranslate a source sentence into a target sentence.\nLet's break down the code step by step:\nOptimizer Definition:\noptimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n\u2022 optim.Adam: This is the Adam optimizer used for training the model. Adam is an adaptive \nlearning rate optimization algorithm that computes individual learning rates for different \nparameters based on the first and second moments of the gradients.\n\u2022 model.parameters(): This refers to the parameters of the model (which is your Transformer \nmodel). These are the weights and biases that will be optimized during training.\n\u2022 lr=0.0001: This is the learning rate, which controls how much the model's weights are adjusted \nwith respect to the loss gradient. A smaller learning rate can lead to more stable training but may \ntake longer to converge.\n\u2022 betas=(0.9, 0.98): These are the beta values used in the calculation of running averages for the \nfirst and second moments of the gradients. In Adam, beta1 (0.9) is typically used for the moving \naverage of the first moment (mean), and beta2 (0.98) is used for the second moment (uncentered \nvariance).\n\u2022 eps=1e-9: This is a small constant added to avoid division by zero during the computation of the \nlearning rate update. It's a safeguard to ensure stability.\nLoss Function Definition:\nPAD_IDX = SRC_VOCAB[PAD_TOKEN]\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n\u2022 PAD_IDX = SRC_VOCAB[PAD_TOKEN]: \n\u25cb This gets the index of the padding token (<pad>) in the source vocabulary (SRC_VOCAB). \nPadding tokens are used to make all sentences in a batch have the same length, but they \ndon't contribute to the actual content of the sentence.\n\u2022 criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX): \n\u25cb CrossEntropyLoss is a commonly used loss function for multi-class classification tasks. In the \ncontext of sequence generation (like translation), it computes the loss for each predicted \ntoken (logits) against the true token labels in the target sequence.\n\u25cb ignore_index=PAD_IDX: This ensures that padding tokens (which do not carry any \nmeaningful information) are ignored during the loss calculation. That means if the model \npredicts a padding token, it won\u2019t contribute to the loss and won't affect training.\nSummary:\n\u2022 The Adam optimizer is used to update the parameters of the Transformer model during training \nwith an appropriate learning rate and gradient moment settings.\n\u2022 The CrossEntropyLoss is used to measure the performance of the model by comparing its \npredictions to the actual target sequence. Padding tokens are ignored during loss computation to \nensure that they don't negatively affect the model's training.\nWith this setup, you can now proceed to train the model by using these components.\nLet's break down the code step by step:\nCollate Function for DataLoader:\ndef collate_fn(batch):\n    src_batch, trg_batch = [], []\n    for sample in batch:\n        src_batch.append(torch.tensor([SRC_VOCAB.get(token, SRC_VOCAB[UNK_TOKEN]) for token in \n   transformer Page 25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3449, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "426f08c2-0273-418a-a0f0-3a6ee9a61fee": {"__data__": {"id_": "426f08c2-0273-418a-a0f0-3a6ee9a61fee", "embedding": null, "metadata": {"page_label": "26", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35d354fc-bc8b-46b6-8c25-6bd0de7c4bbe", "node_type": "4", "metadata": {"page_label": "26", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "b69b2ced6ecf7404917e9bde9deb9c9d5065e5d6496b90905bc6fb6028bf6a65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "src_batch.append(torch.tensor([SRC_VOCAB.get(token, SRC_VOCAB[UNK_TOKEN]) for token in \n[SOS_TOKEN] + sample['src'] + [EOS_TOKEN]]))\n        trg_batch.append(torch.tensor([TRG_VOCAB.get(token, TRG_VOCAB[UNK_TOKEN]) for token in \n[SOS_TOKEN] + sample['trg'] + [EOS_TOKEN]]))\nsrc_batch = pad_sequence(src_batch, padding_value=SRC_VOCAB[PAD_TOKEN])\n    trg_batch = pad_sequence(trg_batch, padding_value=TRG_VOCAB[PAD_TOKEN])\nreturn src_batch.transpose(0, 1), trg_batch.transpose(0, 1)\n\u2022 Purpose: This is a collate function used by the DataLoader to batch the data.\n\u25cb It takes a list of individual samples (each containing source and target sentences) and \nprocesses them into batches.\n\u25cb Padding: It pads the sequences so that all sequences in the batch are the same length. \nPadding is done using the <pad> token from the vocabulary.\n\u2022 Steps:\n\u25cb Add special tokens: Adds SOS_TOKEN and EOS_TOKEN to the source (src) and target (trg) \nsentences.\n\u25cb Convert tokens to indices: The sentences are converted to indices using the SRC_VOCAB \nand TRG_VOCAB vocabularies. If a token is not found, it is replaced with <unk> (unknown \ntoken).\n\u25cb Pad sequences: Sequences are padded using pad_sequence so they are all of equal length \nwithin the batch.\n\u25cb Return: It returns the padded source and target batches, transposed to fit the model's input \nrequirements.\nTraining Function:\ndef train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    print(len(iterator))\n    for src, trg in tqdm(iterator, desc=\"Training\", leave=False):\n        optimizer.zero_grad()\noutput = model(src, trg[:, :-1])\noutput_dim = output.shape[-1]\noutput = output.contiguous().view(-1, output_dim)\n        trg = trg[:, 1:].contiguous().view(-1)\nloss = criterion(output, trg)\nloss.backward()\ntorch.nn.utils.clip_grad_norm_(model.parameters(), clip)\noptimizer.step()\nepoch_loss += loss.item()\nreturn epoch_loss / len(iterator)\n\u2022 Purpose: This function trains the model for one epoch (one pass through the entire dataset).\n\u2022 Steps:\n\u25cb Model in training mode: model.train() puts the model in training mode (so dropout layers, \netc., are active).\n\u25cb Iterate through batches: For each batch (src, trg), it performs the following: \n\u25aa Forward pass: Passes the source and target sequences (excluding the last token in \ntarget, trg[:, :-1]) through the model.\n\u25aa Reshape outputs: The output tensor is reshaped and flattened to calculate the loss for \neach token.\n\u25aa Loss calculation: The loss is calculated using the criterion (CrossEntropyLoss).\n\u25aa Backpropagation: The loss is backpropagated, and the model parameters are \nupdated.\n\u25aa Gradient clipping: To avoid exploding gradients, gradient clipping is applied \n(clip_grad_norm_).\n   transformer Page 26", "mimetype": "text/plain", "start_char_idx": 8, "end_char_idx": 2741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d3415939-c466-4dfe-9018-3d21344c007e": {"__data__": {"id_": "d3415939-c466-4dfe-9018-3d21344c007e", "embedding": null, "metadata": {"page_label": "27", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cbeecace-e833-4199-8735-5a0bb1513094", "node_type": "4", "metadata": {"page_label": "27", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "ca47561df5d2c4b765ec3415270006e1df06c0fe5dd0784ceb7f6a8aea264c47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(clip_grad_norm_).\n\u25aa Update weights: The optimizer steps forward and updates the model's weights.\n\u25cb The function returns the average loss for the epoch.\nEvaluation Function:\ndef evaluate(model, iterator, criterion):\n    model.eval()\n    epoch_loss = 0\n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n            src, trg = batch\noutput = model(src, trg[:, :-1])\noutput_dim = output.shape[-1]\noutput = output.contiguous().view(-1, output_dim)\n            trg = trg[:, 1:].contiguous().view(-1)\nloss = criterion(output, trg)\nepoch_loss += loss.item()\nreturn epoch_loss / len(iterator)\n\u2022 Purpose: This function evaluates the model performance on the validation or test set. It's similar \nto the training loop but without backpropagation.\n\u2022 Steps:\n\u25cb Model in evaluation mode: model.eval() sets the model to evaluation mode (disables \ndropout).\n\u25cb Iterate through validation/test batches: For each batch, it calculates the loss the same way \nas in training, but without updating model weights (no backpropagation).\n\u25cb No gradient computation: torch.no_grad() disables the computation of gradients, which \nsaves memory and computation time during inference.\n\u25cb It returns the average loss for the epoch.\nTranslation Function:\ndef translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):\n    model.eval()\ntokens = [SOS_TOKEN] + tokenize_de(sentence) + [EOS_TOKEN]\nsrc_indexes = [src_vocab.get(token, src_vocab[UNK_TOKEN]) for token in tokens]\nsrc_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\nsrc_mask = model.generate_mask(src_tensor, src_tensor)\nwith torch.no_grad():\n        enc_src = model.encoder_embedding(src_tensor)\n        for enc_layer in model.encoder_layers:\n            enc_src = enc_layer(enc_src, src_mask[0])\ntrg_indexes = [trg_vocab[SOS_TOKEN]]\nfor i in range(max_len):\n        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\ntrg_mask = model.generate_mask(src_tensor, trg_tensor)\nwith torch.no_grad():\n            output = model.decoder_embedding(trg_tensor)\n            for dec_layer in model.decoder_layers:\n                output = dec_layer(output, enc_src, src_mask[0], trg_mask[1])\n            output = model.fc_out(output)\npred_token = output.argmax(2)[:,-1].item()\ntrg_indexes.append(pred_token)\nif pred_token == trg_vocab[EOS_TOKEN]:\n            break\n   transformer Page 27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2372, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9a70d22e-e788-4806-a617-3a8de5a3deb7": {"__data__": {"id_": "9a70d22e-e788-4806-a617-3a8de5a3deb7", "embedding": null, "metadata": {"page_label": "28", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f3147e3-bb89-45f3-96da-190140a2d457", "node_type": "4", "metadata": {"page_label": "28", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "48228f484c60a039c22267535e14e915863729d456d87ce8117eeab2064bfdf3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "break\ntrg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indexes]\nreturn trg_tokens[1:-1]\n\u2022 Purpose: This function translates a given sentence from the source language to the target \nlanguage using the trained model.\n\u2022 Steps:\n\u25cb Tokenize and prepare source sentence: The source sentence is tokenized, and special \ntokens (SOS_TOKEN, EOS_TOKEN) are added. It is then converted into token indices using \nthe src_vocab.\n\u25cb Generate source mask: A mask is generated to prevent attention to padding tokens during \nencoding.\n\u25cb Encoder: The input sequence is passed through the encoder layers.\n\u25cb Initialize target sequence: The target sequence starts with the SOS_TOKEN.\n\u25cb Iteratively generate target tokens: The model generates one token at a time. The target \nsequence is extended by the predicted token.\n\u25cb Stop at EOS: The process stops when the model predicts the EOS_TOKEN.\n\u25cb Convert token indices to words: Finally, the indices of the generated tokens are mapped \nback to words using the target vocabulary.\nSummary:\n\u2022 Collate Function: Prepares batches by adding special tokens and padding sequences.\n\u2022 Training Function: Trains the model by performing forward passes, loss calculation, \nbackpropagation, and optimization.\n\u2022 Evaluation Function: Evaluates the model on validation/test data without updating the model \nparameters.\n\u2022 Translation Function: Uses the trained model to generate translations for a given input sentence.\nThe vocab (vocabulary) is critical because it is the bridge between words (or tokens) and their \ncorresponding integer indices, which the model can process. Let's look at where and how the vocab is \nused:\nWhere is vocab used in the code?\n1. In the Collate Function: The vocab is used when preparing the batches of data (in the collate_fn \nfunction) before passing them into the model.\nsrc_batch.append(torch.tensor([SRC_VOCAB.get(token, SRC_VOCAB[UNK_TOKEN]) for token in \n[SOS_TOKEN] + sample['src'] + [EOS_TOKEN]]))\ntrg_batch.append(torch.tensor([TRG_VOCAB.get(token, TRG_VOCAB[UNK_TOKEN]) for token in \n[SOS_TOKEN] + sample['trg'] + [EOS_TOKEN]]))\n\u25cb Explanation: \n\u25aa SRC_VOCAB.get(token, SRC_VOCAB[UNK_TOKEN]): For each token in the source \nsentence (with added SOS_TOKEN and EOS_TOKEN), we look up the token in the \nSRC_VOCAB dictionary. If the token is not found, we replace it with the <unk> token \nindex (using SRC_VOCAB[UNK_TOKEN]).\n\u25aa TRG_VOCAB.get(token, TRG_VOCAB[UNK_TOKEN]): Similarly, for the target \nsentence, we look up the token in the TRG_VOCAB dictionary. If the token is not \nfound, it\u2019s replaced with the <unk> token index.\n2. During Translation (translate_sentence function): The vocab is used to convert tokens into their \nrespective indices (before feeding them into the model) and then convert the predicted indices \nback into tokens.\nsrc_indexes = [src_vocab.get(token, src_vocab[UNK_TOKEN]) for token in tokens]\n\u25cb Explanation: The sentence is tokenized, and each token (including SOS_TOKEN and \nEOS_TOKEN) is converted into an index using src_vocab (for the source language). This is \nwhat the model understands\u2014numerical representations of words.\nAfter the model predicts indices (for the target language), we use the target vocab to \n   transformer Page 28", "mimetype": "text/plain", "start_char_idx": 12, "end_char_idx": 3256, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "636d12f6-e916-45b9-aa9f-bd3e3503851f": {"__data__": {"id_": "636d12f6-e916-45b9-aa9f-bd3e3503851f", "embedding": null, "metadata": {"page_label": "29", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "729179ed-47b7-4fae-a665-f616a811afd9", "node_type": "4", "metadata": {"page_label": "29", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "316c7054f77c3a8e2d9471204c414fdddb1556bd3427158dcb05016f6648109f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After the model predicts indices (for the target language), we use the target vocab to \nconvert those indices back into words:\ntrg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indexes]\n\u25cb Explanation: This part maps the predicted target token indices (trg_indexes) back to their \ncorresponding words using trg_vocab.\n3. When Training: In the training loop (both train and evaluate functions), vocab is indirectly used in \nthe loss calculation:\nloss = criterion(output, trg)\n\u25cb The output from the model contains predicted indices for the target sequence. These \npredicted indices are compared to the true target indices (trg) to calculate the loss.\n\u25cb The vocab (especially TRG_VOCAB) is used to convert tokens to indices before the model \nprocesses them and to map predicted indices back to tokens during evaluation or \ntranslation.\nWhy is vocab important in this context?\n\u2022 Tokens to Indices: The neural network can only process numerical values, so we need to convert \nwords (tokens) into numerical representations (indices). This conversion is done using vocab.\n\u2022 Handling Unknown Tokens: The vocab allows us to handle unknown words by mapping them to a \nspecial <unk> token.\n\u2022 Decoding the Output: The model generates indices, and the vocab helps us map those indices \nback to readable words for evaluation or translation.\nSummary:\n\u2022 SRC_VOCAB and TRG_VOCAB are used to convert words into indices (before feeding them to the \nmodel) and convert predicted indices back into words (after the model generates them).\n\u2022 In the collate function, vocab helps prepare the batches.\n\u2022 In the training and evaluation loops, vocab helps convert between tokens and indices to calculate \nloss.\n\u2022 In the translation function, vocab helps convert tokens into indices before passing them to the \nmodel and then converts predicted indices back to readable words.\nThe code you\u2019ve shared implements the training loop for training a Transformer-based model for \nmachine translation. Let me break it down for you step by step:\n1. Hyperparameters Setup\nN_EPOCHS = 1\nCLIP = 1.0\nBATCH_SIZE = 32\n\u2022 N_EPOCHS: Number of training epochs (1 in this case). This means the model will go through the \nentire training data once.\n\u2022 CLIP: The gradient clipping value. This helps avoid exploding gradients by limiting the size of \ngradients during backpropagation.\n\u2022 BATCH_SIZE: Number of examples in each batch when training.\n2. Data Loaders\ntrain_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n\u2022 DataLoader: This is used to load the data in batches.\n\u25cb train_dataloader: Loads the training data in batches of size BATCH_SIZE and shuffles the \ndata.\n\u25cb val_dataloader: Loads the validation data in batches. It does not shuffle the data as it's only \nused for evaluation during training.\n\u2022 collate_fn: This is used to prepare batches. It takes care of padding sequences to ensure all \nsentences in a batch are of the same length (since transformer models require fixed-length \n   transformer Page 29", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3131, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "871193e4-41b7-40a4-81bb-f8551cafee12": {"__data__": {"id_": "871193e4-41b7-40a4-81bb-f8551cafee12", "embedding": null, "metadata": {"page_label": "30", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22ad2972-52c0-45df-9571-738f1682ff77", "node_type": "4", "metadata": {"page_label": "30", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}, "hash": "b70d2e7ab012f113de8631998fc0a003b38877a3eee6b98927ebc09e5ca05f8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "sentences in a batch are of the same length (since transformer models require fixed-length \ninputs).\n3. Training Loop (For Each Epoch)\nfor epoch in range(N_EPOCHS):\n    start_time = time.time()\ntrain_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, val_dataloader, criterion)\nend_time = time.time()\n\u2022 start_time and end_time: Measure the time taken to complete one epoch.\n\u2022 The loop runs for the number of epochs defined by N_EPOCHS.\n\u2022 During each epoch: \n\u25cb train_loss: The loss computed on the training data by calling the train function.\n\u25cb valid_loss: The loss computed on the validation data by calling the evaluate function.\n4. Saving the Best Model\nif valid_loss < best_valid_loss:\n    best_valid_loss = valid_loss\n    torch.save(model.state_dict(), 'transformer-translation-model.pt')\n\u2022 If the current valid_loss (validation loss) is lower than the best_valid_loss, the model is saved \nusing torch.save(model.state_dict(), 'transformer-translation-model.pt'). This ensures that the \nbest-performing model (with the lowest validation loss) is saved and can be loaded later for \ninference.\n5. Logging Epoch Information\nepoch_mins, epoch_secs = divmod(end_time - start_time, 60)\nprint(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\nprint(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\nprint(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n\u2022 epoch_mins, epoch_secs: These compute the total time for the epoch (in minutes and seconds).\n\u2022 The loss values are printed for both training and validation sets. \n\u25cb PPL (Perplexity) is printed as math.exp(train_loss) and math.exp(valid_loss). Perplexity is a \ncommon metric for evaluating language models. It's the exponentiation of the cross-entropy \nloss, and a lower perplexity means better performance.\n6. Loading the Best Model\nmodel.load_state_dict(torch.load('transformer-translation-model.pt'))\n\u2022 After training, the best model (the one with the lowest validation loss) is loaded using torch.load. \nThis model is now ready for evaluation or inference.\nKey Takeaways:\n\u2022 Training and Validation Loss: During each epoch, the model is trained on the training set and \nevaluated on the validation set.\n\u2022 Model Saving: The best model (based on validation loss) is saved.\n\u2022 Logging: Loss and perplexity metrics are printed for both training and validation.\n\u2022 Loading the Best Model: After training, the best model is loaded for further evaluation or \ninference.\nIn summary:\nThis training loop trains your Transformer model for a defined number of epochs. It tracks the training \nloss and validation loss, and after each epoch, it saves the best model if it improves on the validation \ndata. The final model with the lowest validation loss is loaded and saved for future use.\n   transformer Page 30", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"7392c46d-5b4d-436e-a7c1-b3283f21c096": {"node_ids": ["7640c971-d974-4a3c-a75b-072b5008ec05"], "metadata": {"page_label": "1", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "f648df64-0a96-4e99-a75f-bc250606c95f": {"node_ids": ["04ccfe3c-9f63-4c80-afbd-de2a307aa69c"], "metadata": {"page_label": "2", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "55b33995-c5a4-4fd0-b273-d624f524fe2b": {"node_ids": ["093521af-dd8f-4619-9022-e5eada65779a"], "metadata": {"page_label": "3", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "bad7ee9f-8397-49c7-aca1-22da2107d17c": {"node_ids": ["e42b80bb-1d8a-4683-b0f2-cb990c8103d6"], "metadata": {"page_label": "4", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "69a74af0-200c-4f3d-9995-8d9461dbe8b1": {"node_ids": ["cf0809ed-8cec-4993-b359-5f6dc8e9c4c1"], "metadata": {"page_label": "5", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "47440797-be04-4c3e-92bf-e6a0f1d16040": {"node_ids": ["a210186a-4ca8-49b8-87e2-273595024777"], "metadata": {"page_label": "6", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "fa508c27-f9ab-4b63-8508-5a1baeb708c4": {"node_ids": ["3d199ae9-9787-41d4-bde5-efd220aee357"], "metadata": {"page_label": "7", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "4f968278-465f-4073-9468-bc9beb3839bb": {"node_ids": ["3048529c-f4aa-4266-bd70-ef21f4f9bc47"], "metadata": {"page_label": "8", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "8288ff90-f499-40b7-bfd9-55647dabd13e": {"node_ids": ["92c049e4-0b98-4564-baf9-c63d186125b3"], "metadata": {"page_label": "9", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "6ca99dcc-f0c1-49d3-8aeb-180b877f2a00": {"node_ids": ["8476e1b3-b9cc-427f-bd26-dae9013263d4"], "metadata": {"page_label": "10", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "772b9db4-473c-490d-b7bd-8dd33cf61f11": {"node_ids": ["d844248f-f515-4da4-94e1-26e939335820"], "metadata": {"page_label": "11", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "4a2cda1d-1a27-48a1-ba21-2be43b90da91": {"node_ids": ["1558bd5a-19c5-4ce9-8a51-a391588ddfcd"], "metadata": {"page_label": "12", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "96b53d13-6ac1-40da-aa24-1f8afe0ecfcb": {"node_ids": ["fc731157-a527-4712-be13-e352c3567a31"], "metadata": {"page_label": "13", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "b78ea85e-05c7-4b25-86fb-89793ca403c8": {"node_ids": ["aed5196b-e68a-47f4-b9a8-f59785ccce55"], "metadata": {"page_label": "14", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "28e732bb-d22b-47a5-b332-a8b6634faf14": {"node_ids": ["e6887d47-75c4-4050-8d98-53fe3e232c13"], "metadata": {"page_label": "15", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "e7020f88-8394-467b-9b6a-5c611ef616e2": {"node_ids": ["7a58867c-9dfd-4b7b-ac50-14bc9b1fdabc"], "metadata": {"page_label": "16", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "adbfeec2-5a5f-4ff4-9aef-1474496dc35e": {"node_ids": ["25439d0e-a06a-4b07-83e2-944bdca8690e"], "metadata": {"page_label": "17", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "10a790e5-5165-4b89-bd06-8e554e694b9d": {"node_ids": ["13e9062c-84eb-4aba-bde6-87032e8411c3"], "metadata": {"page_label": "18", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "4611a9e2-15bf-402e-9373-af165e09db4c": {"node_ids": ["c43d09d4-188a-436f-ac94-8d6ddef65372"], "metadata": {"page_label": "19", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "23cc4249-e09f-49b7-9dc6-02d5669dfc39": {"node_ids": ["cc8f10f8-93c8-45b1-8e96-909ea9271f51"], "metadata": {"page_label": "20", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "1c34b811-bb5c-4b74-a7c8-5132e5f0fbd8": {"node_ids": ["8972e66c-a028-4e86-829c-ec2c5bf17daf"], "metadata": {"page_label": "21", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "523d422c-6f89-49f0-8548-2ceb6209177a": {"node_ids": ["542e88c8-14d9-4d10-962b-9b56f7afec3c"], "metadata": {"page_label": "22", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "f22d37ed-ada0-4778-9e4d-4588c832f236": {"node_ids": ["bc71aea8-4100-4845-bb25-59571125e140"], "metadata": {"page_label": "23", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "808a7ae9-6b1a-41ee-b194-63087b7fdcb7": {"node_ids": ["0b825b6d-a0c3-4b11-9f93-c4a863d4a2ea"], "metadata": {"page_label": "24", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "8805232e-3503-418b-b19e-e81313da17ec": {"node_ids": ["8081dad5-b35a-4ea0-83d0-9c803016a5de"], "metadata": {"page_label": "25", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "35d354fc-bc8b-46b6-8c25-6bd0de7c4bbe": {"node_ids": ["426f08c2-0273-418a-a0f0-3a6ee9a61fee"], "metadata": {"page_label": "26", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "cbeecace-e833-4199-8735-5a0bb1513094": {"node_ids": ["d3415939-c466-4dfe-9018-3d21344c007e"], "metadata": {"page_label": "27", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "6f3147e3-bb89-45f3-96da-190140a2d457": {"node_ids": ["9a70d22e-e788-4806-a617-3a8de5a3deb7"], "metadata": {"page_label": "28", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "729179ed-47b7-4fae-a665-f616a811afd9": {"node_ids": ["636d12f6-e916-45b9-aa9f-bd3e3503851f"], "metadata": {"page_label": "29", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}, "22ad2972-52c0-45df-9571-738f1682ff77": {"node_ids": ["871193e4-41b7-40a4-81bb-f8551cafee12"], "metadata": {"page_label": "30", "file_name": "transformer.pdf", "file_path": "f:\\Llama_index_Project\\Study_Material\\..\\Study_Material\\data\\transformer.pdf", "file_type": "application/pdf", "file_size": 4445724, "creation_date": "2025-05-10", "last_modified_date": "2025-02-02"}}}}